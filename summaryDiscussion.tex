\section{Evaluation}
\label{sec:SummaryDiscussion}
The goal of this thesis was to segment road pixels from aerial imagery, using a convolutional neural network. This network shares many similarities to the networks used by \cite{Mnih_aerial_images_noisy} and \citep{MnihThesis}. Experiments demonstrated that this architecture trained on the Massachusetts Roads Dataset achieved an averaged breakeven point of 0.8494. This is a bit below comparable results from other works. The most likely explanation is that the default network architecture constrained the capacity of the model. From the qualitative analysis, the classifier seemed to have generalized fairly well to the task of road segmentation.\\

 The research questions of this thesis are related to this goal. Automatically generated aerial imagery datasets often suffer from label noise, and the research questions involve methods which can possibly alleviate the negative effects of inconsistent labelling. This section will attempt to resolve the research questions defined in Section \ref{sec:Goals and Research Questions} by a brief discussion based on the results from Chapter \ref{cha:ResearchAndResults}.
 
\begin{description}[ style=nextline, leftmargin=1.5em, rightmargin=1.5em]
\item[Research question 1:]{\it Does the bootstrapping loss function give a significant improvement of precision and recall for datasets with noisy labels?}
\end{description}

\cite{Mnih_aerial_images_noisy} showed that performance could be improved for aerial imagery, by having the loss function model the noise distribution. They also found two particular breeds of label noise in aerial imagery datasets, which they called omission and registration noise. The bootstrapping loss function proposed by \cite{Reed_noisy_labels_bootstrapping}, has also showed promising results for several noisy datasets. In this thesis, this particular loss function was therefore tested on aerial imagery, to see if it provided robustness towards omission and registration noise. Furthermore, a proposed variation of bootstrapping was also tested.\\

The experiments demonstrated that the bootstrapping method did provide some robustness towards label noise. For increasing levels of omission noise the gap in performance between the cross-entropy and the bootstrapping loss function was increasing. However, the results were not statistically significant \todo{Need to use more caution. The results were not significant}. The loss function was not tested on artificially increasing levels of registration noise, which would make the results more prominent. This would require some sort of image morphing, and local skewing of the roads present in the label images. \\

Tests performed on the Norwegian Roads Dataset N50 showed that confident bootstrapping performed slightly better than bootstrapping, and that both bootstrapping methods did somewhat better than the baseline. In summary, the bootstrapping loss function seemed to have a positive effect on noisy labels. Unfortunately, the effect was not statistically significant. 

\begin{description}[ style=nextline, leftmargin=1.5em, rightmargin=1.5em]
\item[Research question 2:]{\it How can curriculum learning improve results in deep learning, and does this improve precision and recall for aerial images?}
\end{description}

Curriculum learning proposed by \cite{Bengio_curriculumlearning}, demonstrated compelling results for several tasks. The method increased generalization accuracy and reduced convergence, by organizing each dataset according to the the examples difficulty. The classifier is trained by gradually introducing harder examples to the training set. However, the curriculum strategies used for sorting the dataset were specific for each task, and was not particularly adaptable to road detection. This challenge was addressed by \ac{SPL} \citep{Kumar_self_paced_learning}, where the curriculum strategy is internalized in the classifier. The classifier simultaneously calculate loss and and adjust the contribution of each example based on it's easiness  \todo{Wrong, based on what criteria}.\\

The proposed curriculum strategy in this thesis, requires no modification to the classifier. Instead, the difficulty of examples are estimated based on inconsistencies between labels and predictions, and a curriculum dataset build based on these estimates. A limitation of this method is that it's effectiveness is based on the predictions generated from a curriculum teacher model, which has to be trained beforehand. However, the method is not task specific and can probably be applicable to any domains. This curriculum strategy can also be applied with any supervised algorithm, without modifying the core algorithm. \\

The experiments demonstrated consistently better generalization accuracy using curriculum learning. The resulting precision and recall curves from experiments conducted with both the Norwegian Roads Dataset and Massachusetts Roads Dataset, showed that the network trained on the curriculum datasets performed better than the baseline datasets. The experiments used a two stage training set, with the first stage containing examples with low difficulty estimates. The entire training set was switched mid-training. \\

The baseline and curriculum tests, had the same network configuration and the same second stage difficulty distribution. The only variable between the tests, was the content of the first stage. The experiments revealed that the first stage, did impact the final accuracy of the model. The advantage in performance of a simple first stage did not vanish in the trained model. This might indicate that the examples presented early, exert a larger influence on the outcome, than the examples presented later in the optimization. This is also evident from results of anti-curriculum learning which converged to a generalization accuracy below the baseline. \\

However, whether purely removing inconsistent labels is better than delaying the presentation of inconsistent labels, is unclear. Curriculum learning is at least a safer approach, since it does not accidentally remove correct examples that are simply very hard. The switch between stages, also lead to an immediate and significant increase in test loss. This was alleviated by gradually mixing in harder examples, by utilizing smaller subsequent stages.\\

There is also an uncertainty about whether the advantage of curriculum learning is present for very large training sets. The size of the training set was limited to around 200 000 examples, because of runtime concerns. Experiments did show that curriculum learning can be advantageous for domains where a limited amount of data is available. Increased accuracy with curriculum learning was observed, when the teacher model had been trained with a training set of limited size. The gradual curriculum learning approach even outperformed the teacher model, in terms of precision and recall.\\

In conclusion, improved precision and recall for curriculum learning was observed for both aerial imagery datasets. The curriculum strategy of measuring inconsistency between labels and predictions works well, and can easily be combined with deep learning. However, this does require training a teacher model beforehand.


