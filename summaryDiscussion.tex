\section{Evaluation}
\label{sec:SummaryDiscussion}
The goal of this thesis was to segment road pixels from aerial imagery using a convolutional neural network. This network shares many similarities to the networks employed by \cite{Mnih_aerial_images_noisy} and \citep{MnihThesis}. Experiments demonstrated that this architecture trained on the Massachusetts Roads Dataset achieved an averaged precision and recall breakeven point of 0.8494. This is a bit below comparable results from other works. The most likely explanation is that the network architecture constrained the capacity of the model. From the qualitative analysis, the classifier seemed to have generalized fairly well to the task of road segmentation.\\

 The research questions of this thesis are related to this goal. Automatically generated aerial image datasets often suffer from label noise, and the research questions involve methods which can possibly alleviate the negative effects of inconsistent labelling. This section will attempt to resolve the research questions defined in Section \ref{sec:Goals and Research Questions} by a brief discussion based on the results from Chapter \ref{cha:ResearchAndResults}.
 
\begin{description}[ style=nextline, leftmargin=1.5em, rightmargin=1.5em]
\item[Research question 1:]{\it Does the bootstrapping loss function give a significant improvement of precision and recall for datasets with noisy labels?}
\end{description}

\cite{Mnih_aerial_images_noisy} showed that performance could be improved for aerial imagery, by having the loss function model the noise distribution. They also found two particular breeds of label noise in aerial image datasets, which they called omission and registration noise. The bootstrapping loss function proposed by \cite{Reed_noisy_labels_bootstrapping} has also shown promising results for several noisy datasets. In this thesis, this particular loss function was therefore tested on aerial imagery, to see if it provided robustness towards omission and registration noise. Furthermore, a proposed variation of bootstrapping was also tested.\\

The experiments demonstrated that the bootstrapping method did provide some robustness towards label noise. For increasing levels of omission noise, the gap in performance between the cross-entropy and the bootstrapping loss function seemed to be somewhat increasing. However, the results were not statistically significant. The loss function was not tested on artificially increasing levels of registration noise, which could make the results more convincing. This would require some sort of image morphing, and local skewing of the roads present in the label images. \\

Tests performed on the Norwegian Roads Dataset N50 showed that both bootstrapping methods had precision and recall values slightly better than the baseline. In summary, the bootstrapping loss function seemed to have a slightly positive effect on noisy labels. Unfortunately, the effect was not statistically significant for increasing levels of omission noise. 

\begin{description}[ style=nextline, leftmargin=1.5em, rightmargin=1.5em]
\item[Research question 2:]{\it How can curriculum learning improve results in deep learning, and does this improve precision and recall for aerial images?}
\end{description}

Curriculum learning proposed by \cite{Bengio_curriculumlearning} demonstrated compelling results for several tasks. The method increased generalization accuracy and resulted in faster convergence, by organizing each dataset according to the estimated difficulty of each example. The classifier is trained by gradually introducing harder examples to the training set. However, the curriculum strategies used for sorting the dataset were specific for each task, and was not particularly adaptable to road detection. This challenge was addressed by \ac{SPL} \citep{Kumar_self_paced_learning}, where the curriculum strategy is internalized in the model. The model simultaneously optimizes the objective function and determines what examples to consider at each iteration. Examples that are easily predicted are considered ``easy" by this approach.\\

The proposed curriculum strategy in this thesis does not alter the classifier algorithm. Instead, the example difficulties are estimated based on inconsistencies between labels and predictions. A curriculum dataset is built based on these estimates. The examples of the curriculum dataset are then mixed into the training set during training. A limitation of this method is that its effectiveness is based on the predictions generated from a curriculum teacher classifier, which has to be trained beforehand. However, the method is not task specific and can probably be applicable to any domains. This curriculum strategy can also be applied to any supervised algorithm, since the strategy primarily affects the dataset and not the algorithm. \\

The experiments demonstrated consistently better generalization accuracy using curriculum learning. The resulting precision and recall curves from experiments conducted with both the Norwegian Roads Dataset and Massachusetts Roads Dataset, showed that the network trained on the curriculum datasets performed better than the baseline datasets. The experiments used a two stage training set, with the first stage containing examples with low difficulty estimates. The entire training set was replaced mid-training. \\

The baseline and curriculum tests had the same network configuration and the same second stage difficulty distribution. The only variable between the tests was the content of the first stage. The experiments revealed that the first stage did impact the final accuracy of the classifier. The performance advantage of a simple first stage did not vanish in later stages of training. This might indicate that the examples presented early, exert a larger influence on the outcome, than the examples presented later in the optimization process. This is also evident from results of anti-curriculum learning which converged to a generalization accuracy below the baseline. \\

However, whether purely removing inconsistent labels is \todo{are?} better than delaying the presentation of inconsistent labels, are \todo{is?} unclear. Curriculum learning is at least a safer approach, since it does not accidentally remove correct examples that are simply very hard. The switch between stages, also resulted in an immediate and significant increase in test loss. This was alleviated by gradually mixing in harder examples.\\

There is also an uncertainty about whether the advantage of curriculum learning is present for very large training sets. The size of the training set was limited to around 200000 examples, because of runtime concerns. Experiments did show that curriculum learning can be advantageous for domains where a limited amount of data is available. Increased accuracy with curriculum learning was observed when the teacher model had been trained with a training set of limited size. The gradual curriculum learning approach even outperformed the teacher classifier in terms of precision and recall.\\

In conclusion, improved precision and recall for curriculum learning was observed for both aerial image datasets. The curriculum strategy of measuring inconsistency between a label and a prediction works well, and can easily be combined with deep learning. However, this does \todo{do?} require training a teacher classifier beforehand.


