\section{Bootstrapping loss}
\label{sec:bootstrapping_loss}
Normally, the loss function calculates the loss assuming the labels are correct. But many datasets contain label noise, which will result in the learner being penalized for making a correct prediction if the label happens to be inconsistent. According to \cite{Reed_noisy_labels_bootstrapping}, tweaking the loss function by incorporating the network's own predictions can yield improved robustness to inconsistent labelling. Whereas other approaches \citep{Mnih_aerial_images_noisy}\citep{Sukhbaatar_noisy_network_learning} explicitly model the noise distribution, the proposed loss function utilizes the implicit knowledge acquired by the network during optimization. The bootstrapping method, computes the loss by a convex combination of the model prediction q and the label y. Additionally, this thesis propose a variation of the bootstrapping loss function, which only incorporate confident predictions. \\

The bootstrapping loss function, use the current model prediction $q$ and the noisy training label $y$ in a convex combination, which result in a modified target. Except for this, the loss function is similar to cross entropy\todo{Is it though?}. The model prediction's contribution to the convex combination is decided by the $\beta$ parameter. The bootstrapping loss for the aerial road detection system is defined below:

 \begin{flalign*}
  \mathcal{L}_{\text{hard}}(q,y) =&  - \sum\limits_{i=1}^{w_m^2} [\beta y_i + (1-\beta)\mathbb{1}_{q_i > 0.5}]\log(q_i)  \\
                    & - \sum\limits_{i=1}^{w_m^2} [\beta (1-y_i) + (1-\beta)(1-\mathbb{1}_{q_i > 0.5})]\log(1 - q_i) 
 \end{flalign*}

\noindent where $\mathbb{1}_{q_i > 0.5}$ is the MAP estimate of the model prediction $q$ \todo{Why is this important?}. For the task of road detection it is simply a threshold operation. All probabilities above 0.5 are set to 1, otherwise 0. This is the reason why this loss function is denoted hard. A soft version of bootstrapping where the predictions $q_i$ are used directly, has also tested by \cite{Reed_noisy_labels_bootstrapping} , but generally performed worse than the bootstrapping hard variant.\\

Bootstrapping loss combined with gradient descent, results in an EM-like algorithm. In the E-step the modified targets are generated, whereas in the M-step the network weights are adjusted to better predict the modified targets. The goal is for the learner to rely less on the inconsistent labels, and develop more consistent implicit knowledge, which in turn further improves the quality of the modified targets.\\
 
Since, the task involves the binary task of discriminating road pixels from non-road pixels, a slightly modified version of bootstrapping is also tested in the thesis. This approach is named confident bootstrapping, since all model predictions between 0.2 and 0.8 are ignored. Only predictions that the learner have a high confidence in, are allowed to contribute in the convex combination. An added benefit of this loss function, is the possibility of increasing the factor $\beta$. The confident bootstrapping loss function is denoted:

  \begin{flalign*}
  \mathcal{L}_{\text{confident}}(q,y) =&  - \sum\limits_{i=1}^{w_m^2} [\beta y_i + (1-\beta)\mathbb{1}_{q_i > 0.8}]\log(q_i)  \\
                    & - \sum\limits_{i=1}^{w_m^2} [\beta (1-y_i) + (1-\beta)(\mathbb{1}_{q_i < 0.2})]\log(1 - q_i) 
 \end{flalign*}
 
\noindent where $\mathbb{1}_{q_i > 0.8}$ and $\mathbb{1}_{q_i < 0.2}$ are threshold operations which only keep fairly confident predictions. For the task of road detection, this translates to saying pixel predictions above 0.8 are most likely road pixels, and predictions below 0.2 are likely to be non-road pixels.\\

In the actual implementation, the $\beta$ parameter is annealed from the max value $\beta_{max}$, down to the minimum value of $\beta_{min}$, starting from $M$ epochs. This is because the learner should have some implicit knowledge before using it's predictions to modify the target. This is similar to the approach \todo{Just a bit of the approach. The method entirely different} by \cite{Sukhbaatar_noisy_network_learning}. The configurable parameters for bootstrapping can be found in Table \ref{tab:bootstrapping_parameters}

\begin{table}[htp]
\caption{Hyperparameters for bootstrapping loss.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^l ^p{3.5cm}}\hline
\rowstyle{\bfseries}
 		 Parameter & Description & Value\\\hline
 		 Cost function & Modified cost function  & bootstrapping or confident boostrapping \\
 		 $\beta_{max}$ & Mix factor  & 1.0 \\
 		 $\beta_{min}$ & Minimum mix factor & 0.90 \\
 		 $M$ & When to start mixing & 60 \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:bootstrapping_parameters}
\end{table}