\section{Bootstrapping loss}
\label{sec:bootstrapping_loss}
\todo[inline]{This is unusually bad:-(((}
Normally, the \ac{CNN} is optimized with a cross entropy loss function\todo{Weird way to start, and cross entropy normally???}, which assume the labels are correct \todo{Bad apple}. According, to \cite{Reed_noisy_labels_bootstrapping} tweaking the loss function by incorporating the network's own predictions can yield improved robustness to inconsistent labelling. Whereas other approaches \citep{Mnih_aerial_images_noisy}\citep{Sukhbaatar_noisy_network_learning} explicitly model the noise distribution, the proposed loss function utilize the implicit knowledge acquired by the network during optimization. The bootstrapping method, compute the loss by a convex combination of the model prediction q and the label y. Additionally, the thesis propose a variation of bootstrapping loss function, which only incorporate confident predictions. \\

The bootstrapping loss function, use the current model's prediction $q$ and the noisy training label $y$ in a convex combination, which result in a modified target. Except for this, the loss function is similar to cross entropy\todo{Is it though?}. The contribution to the convex combination from the model's prediction is decided by the $\beta$ parameter. The bootstrapping loss for the aerial road detection system is defined below:

 \begin{flalign*}
  \mathcal{L}_{hard}(q,y) =&  - \sum\limits_{i=1}^{w_m^2} [\beta y_i + (1-\beta)\mathbb{1}_{q_i > 0.5}]log(q_i)  \\
                    & - \sum\limits_{i=1}^{w_m^2} [\beta (1-y_i) + (1-\beta)(1-\mathbb{1}_{q_i > 0.5})]log(1 - q_i) 
 \end{flalign*}

\noindent where $\mathbb{1}_{q_i > 0.5}$ is the MAP estimate of the model's prediction $q$ \todo{Why is this important?}. For the task of road detection, this is simply a threshold operation. All probabilities above 0.5 are set to 1, otherwise 0. This is the reason why it is denoted hard \todo{Superbad}\\

Bootstrapping loss combined with gradient descent, lead to an EM-like algorithm. In the E-step the modified targets are generated, whereas in the M-step the network weights are adjusted to better predict the modified targets. The goal is for the learner to rely less on the inconsistent labels, and develop more consistent implicit knowledge, which in turn further improve the learners predictions \todo{This does not make sense, except for the part about EM}
 
Since, the task involve binary task of discriminating road pixels from non-road pixels, a slightly modified version of bootstrapping is used in the thesis. This approach is named confident bootstrapping, since all model predictions between 0.2 and 0.8 are ignored. Only predictions that the learner are confident about, are allowed to contribute in the convex combination. An added benefit of this loss function, is the possibility of increasing the factor $\beta$. The confident bootstrapping loss function is denoted:

  \begin{flalign*}
  \mathcal{L}_{confident}(q,y) =&  - \sum\limits_{i=1}^{w_m^2} [\beta y_i + (1-\beta)\mathbb{1}_{q_i > 0.8}]log(q_i)  \\
                    & - \sum\limits_{i=1}^{w_m^2} [\beta (1-y_i) + (1-\beta)(\mathbb{1}_{q_i < 0.2})]log(1 - q_i) 
 \end{flalign*}
 
\noindent where $\mathbb{1}_{q_i > 0.8}$ and $\mathbb{1}_{q_i < 0.2}$ are threshold operations which only keep fairly confident predictions. For the task of road detection, this translates to saying pixel predictions above 0.8 are most likely road pixels, and predictions below 0.2 are very likely to be not be a road pixel.\\

In the implementation, the $\beta$ parameter is annealed often from 1.0, down to the minimum value of $\beta_{min}$, after $\beta_{start}$ epochs. This is because the learner should have some implicit knowledge before using it's predictions to modify the target. Similar to \cite{Sukhbaatar_noisy_network_learning}. This is probability not important for confident bootstrapping \todo{OMG, so bad}. The configurable parameters for bootstrapping can be found in Table \ref{tab:bootstrapping_parameters}

\begin{table}[htp]
\caption{Hyperparameters for bootstrapping loss}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^l ^p{3.5cm}}\hline
\rowstyle{\bfseries}
 		 Parameter & Description & Value\\\hline
 		 Cost function & Modified cost function  & bootstrapping or confident boostrapping \\
 		 $\beta$ & Mix factor  & 1.0 \\
 		 $\beta_{min}$ & Minimum mix factor & 0.90 \\
 		 $\beta_{start}$ & When to start mixing & 60 \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:bootstrapping_parameters}
\end{table}