\section{Contributions}~\label{cont}
\label{sec:Contributions}
This thesis sought to test approaches for dealing with noisy labels in real-world datasets. This is an compelling inquiry in the field of machine learning, where the trend of using deep neural networks with a huge number of adjustable parameters, requires large training sets to generalize well. There is an abundance of existing data available online, which can be used for learning. Unfortunately, in many cases this data lacks accurate labels for supervised training. To manually label the data can be expensive, and very time consuming in many domains, such as transcribing speech for speech recognition, and tracing ground truth for semantic segmentation. Automatically generating datasets from existing data sources is a quick and economical solution, but can result in datasets with a lot of label noise.\\

The problem of label noise, was therefore tackled in this thesis by testing two different methods. Bootstrapping modifies the loss function, in order to reduce the impact of inconsistent labels. Curriculum learning modifies the training regime by sorting the training set into stages from "easy" to "hard" examples. The sorting mechanism, or curriculum strategy is based on measuring inconsistencies between label and teacher predictions. Coincidentally, the "hard" examples often have inconsistent labelling, and are presented at a later stage of optimization. In effect, inconsistent examples are less frequent concurrence in the first stage of the curriculum dataset.\\
 
The curriculum strategy can most likely be applied to tasks in other domains, since there is not anything intrinsic to images used to estimate example difficulty. However, the effectiveness of this curriculum strategy has not been verified for other domains than road detection in aerial images.\\

The thesis found that bootstrapping did show some robustness towards label noise. The effect was however not significant. Additionally, the base network also performed surprising well for very high rates of omission noise.\\

Curriculum learning demonstrated consistently improved generalization accuracy in the experiments. The improved accuracy, was observed both for the Massachusetts Roads Dataset, as well as the Norwegian Roads Dataset. Changing the example distribution of only the first stage training set, affected the final outcome of the training procedure.\\

