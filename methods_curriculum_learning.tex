\section{Curriculum learning}
\label{sec:curriculum_learning}
The curriculum learning method presented by \cite{Bengio_curriculumlearning}, showed that presenting the dataset in a certain order yielded better generalization as well as faster convergence. Similar to a school curriculum, easier concepts are taught before harder concepts, which most likely is the preferred way of learning for humans \citep{Khan_human_teach}. This translates to having the learning algorithm do optimization on a dataset consisting of "easier" examples, before introducing "harder" examples. Continuing this analogy, there is a teacher who decides the curriculum by judging how easy concepts is to grasp. For some datasets, such as language modelling datasets, the role of the teacher can easily be performed by a human. In language modelling tasks, a viable curriculum strategy is to start training using a simple dataset containing examples with only a limited vocabulary, and then gradually grow the allowed vocabulary. However, similar curriculum strategies are harder to identify for datasets involving images. The solution is therefore to outsource the job of curriculum teacher, to a supervised learning algorithm.  \\

In this thesis, aerial image datasets are used. Based purely on color image patches, a supervised learning algorithm should be able to detect and segment roads found in these patches. A curriculum strategy involving this type of data is harder to identify. Is curved roads segment harder to learn for a machine learning algorithm than a straight road segment? Or are roads in rural scenes easier to learn than roads in an urban setting? What configurations of pixel intensities are easier? It is hard for a human to identify features in images which a machine learning algorithm would consider easier to learn. \\


Hence, a classifier is first trained on the available examples and given the role of the teacher. The classifier, regardless of competence, will generate predictions which is used in a difficulty estimation of each example. The difficulty estimation is simply based on the amount of disagreement between the teacher's prediction and the label of the example. The difficulty estimator is defined below:  \\

 \begin{flalign*}
  &  d(y, q) = \frac{1}{w_m^2}\sum_{i=0}^{w_m^2} |y_i - \mathbb{1}_{q_i > r} |,  \\
 \end{flalign*}
 
 
\noindent where $q$ is the curriculum teacher's prediction, $y$ is the label. The $\mathbb{1}_{q_i > s}$ is a threshold operator, where $r$ is the threshold value which gives the best precision and recall breakeven for the curriculum teacher classifier. The operator essentially, creates a binary image patch from the prediction probabilities. The estimator compute the average value from the $w_m \times w_m$ patch of label pixels and prediction probabilities. Even though the estimator has been customized to the patch-based approach of aerial road detection, the method can easily be adopted to other types of datasets. This curriculum strategy can therefore be considered generally applicable. The disagreement between a prediction created by a trained classifier and a label, can probably be computed for most datasets. \\

An interesting thing to note, is that a very competent teacher classifier, computing a high difficulty score for an example, might indicate that the example's label is inconsistent.   \\

The implementation of curriculum learning, involve $N$ training set stages. Each stage $\theta$ only contains examples where $d(y, q) < D_{\theta}$, in which the difficulty threshold $ D_{\theta} > D_{\theta -1}$ for all stages $ \theta > \theta -1$. Each subsequent stage, therefore contain examples with a broader range of difficulties \todo{Weird}. In the final stage $N$, the threshold $D_{N}$ is typically set to 1, which result in every example being added to the patch dataset. This stage has a training distribution equal to what a patch dataset created from random sampling would have had.\\

The supervised learning algorithm, is gradually exposed to harder examples by stage switching. The classifer is first optimized with the examples in the first stage, which has the lowest difficulty threshold. At epoch $t_{start}$, the next stage replaces the existing training set by random sampling without replacement. Following stages are mixed into the training set every $t_{stage}$ epoch, until the model has trained with examples from all stages. The hyperparameters and default values for curriculum learning are listed in  Table \ref{tab:curriculum_parameters}. \\

\begin{table}[htp]
\caption{Hyperparameters for curriculum learning.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^l ^l}\hline
\rowstyle{\bfseries}
 		 Parameter & Description & Value\\\hline
 		 $N$ & Number of stages & 2 \\
 		 $D_\theta$ & Threshold value for difficulty estimate for stage $\theta$ & 0.25, 1.0 \\
 		 $t_{start}$ & What epoch to load stage 1 & 50 \\
 		 $t_{stage}$ & Load subsequent stage after every $t_{stage}$ epoch & 50 \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:curriculum_parameters}
\end{table}