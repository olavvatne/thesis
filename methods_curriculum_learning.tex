\section{Curriculum learning}
\label{sec:curriculum_Learning}
The curriculum learning method presented by \cite{Bengio_curriculumlearning} showed that presenting the dataset in a certain order yielded better generalization as well as faster convergence. Similar to an educational curriculum, easier concepts are taught before harder concepts, which most likely is the preferred way of learning for humans \citep{Khan_human_teach}. This translates to having the learning algorithm do optimization on a dataset consisting of ``easier" examples, before introducing ``harder" examples. Continuing this analogy, there is a teacher who decides the curriculum by judging how easy concepts are to grasp. For some datasets, such as language modelling datasets, the role of the teacher can easily be performed by a human. In language modelling tasks, a viable curriculum strategy is to start training using a simple dataset containing sentences with only a limited vocabulary. During training, the vocabulary is gradually increased, which will allow more complex sentences to enter the training set. However, similar curriculum strategies are harder to identify for \todo{check} datasets involving images. The solution is therefore to outsource the job of curriculum teacher to a supervised learning algorithm.  \\

This thesis utilizes aerial image datasets. Based purely on color image patches, a supervised learning algorithm should be able to detect and segment roads found in these patches. A curriculum strategy involving this type of data is harder to identify. For instance, is curved road segments harder to learn for a machine learning algorithm than a straight road segment? Or are roads in rural scenes easier to learn than roads in an urban setting? What configurations of pixel intensities are easiest? It is hard for a human to identify features in images that a machine learning algorithm would consider easier to learn. \\


Hence, a classifier is first trained on the available examples and given the role of the teacher. The classifier, regardless of competence, will generate predictions that are used in a difficulty estimation of each example. The difficulty estimation is simply based on the amount of disagreement between the teacher's prediction and the example's label. The difficulty estimator is defined below:  \\

 \begin{flalign*}
  &  d(y, q) = \frac{1}{w_m^2}\sum_{i=0}^{w_m^2} |y_i - \mathbb{1}_{q_i > r} |,  \\
 \end{flalign*}
 
 
\noindent where $q$ is the curriculum teacher's prediction, and $y$ is the label. The $\mathbb{1}_{q_i > r}$ is a threshold operator, where $r$ is the threshold value which gives the best precision and recall breakeven for the curriculum teacher classifier. The operator essentially creates a binary image patch from the prediction probabilities. The estimator computes the average value from the $w_m \times w_m$ patch of label pixels and prediction probabilities. Even though the estimator has been customized to the patch-based approach of aerial road detection, the method can easily be adapted to other types of datasets. This curriculum strategy can probably be considered generally applicable. The disagreement between a prediction created by a trained classifier and a label can probably be computed for most datasets. \\

An interesting thing to note is that a very competent teacher classifier, computing a high difficulty score for an example, might indicate that the example's label is inconsistent.   \\

The implementation of curriculum learning involves $N$ training set stages. Each stage $\theta$ only contains examples where $d(y, q) < D_{\theta}$, in which the difficulty threshold $ D_{\theta} \geq D_{\theta -1}$ for all stages $ \theta > \theta -1$. Each subsequent stage, therefore, contains examples with a wider range of difficulty estimates. In the final stage $N$, the threshold $D_{N}$ is typically set to 1, which results in every example being added to the patch dataset. This stage has a training distribution equal to what a patch dataset created from random sampling would have had.\\

The supervised learning algorithm is gradually exposed to harder examples by stage switching. The classifer is first optimized with the examples in the first stage, which has the lowest difficulty threshold. At epoch $t_{start}$, the next stage replaces the existing training set by assigning each new example to a random index in the training set. The indices are sampled without replacement. Following stages are mixed into the training set every $t_{stage}$ epoch, until the model has trained with examples from all stages. The hyperparameters and default values for \todo{check} curriculum learning are listed in  Table \ref{tab:curriculum_parameters}. \\

\begin{table}[htp]
\caption[Hyperparameters for curriculum learning]{Hyperparameters for curriculum learning.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^l ^l}\hline
\rowstyle{\bfseries}
 		 Parameter & Description & Value\\\hline
 		 $N$ & Number of stages & 2 \\
 		 $D_\theta$ & Threshold value for difficulty estimate for stage $\theta$ & 0.25, 1.0 \\
 		 $t_{start}$ & What epoch to load stage 1 & 50 \\
 		 $t_{stage}$ & Load subsequent stage after every $t_{stage}$ epoch & 50 \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:curriculum_parameters}
\end{table}