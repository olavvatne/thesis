\section{Experimental Setup}
\label{sec:experimentalSetup}
The network configuration for most experiments are listed in Table \ref{tab:network_parameters}. Any deviations from this configuration will be detailed in this section. In addition, the specific hyperparameters used for each experiment can be found at \url{http://interface.ml/experiments}. Descriptions about tools used for conducting the experiments can be found in Appendix \ref{app:tools}. The relevant parameters for each experiment are detailed below.\\ 

\subsubsection{E1 - Curriculum Learning with Norwegian Roads Dataset}
This experiment involved comparing two patch datasets generated from the Norwegian Roads Dataset. Both datasets have $N=2$ stages, where each stage includes 110 000 training examples. The first patch dataset was created according to a curriculum strategy, where the difficulty estimate $d(y, q)$ is less than a threshold $D_\theta$. The first stage consists of only examples with a difficulty estimate below 0.25,  while the second stage has a threshold of 1.0, which is equivalent to random sampling of patches. The second patch dataset has a threshold $D_\theta$ of 1.0 for both stages, and constitutes the baseline. Switching between the first and second stage, happened by entirely replacing the training set with examples from the second stage \todo{A bit hard to past tense it!}.\\

The curriculum teacher, which generates predictions for the difficulty estimation are a previous trained model. The teacher classifier was trained with a dataset consisting of 440000 examples for 175 epochs. Otherwise, the network parameters closely resembles the default parameters listed in Table \ref{tab:network_parameters} \todo{True?}. The classifier's final \ac{MSE} test loss was 0.0222, and the relaxed precision and recall breakeven was around 0.71. \\

The default network configuration was used for networks trained on both the curriculum dataset and the baseline dataset. Essentially, the only real difference between the two, is the first stage of the datasets. The models were trained for 120 epochs, with a stage switch at epoch 50. Important parameters for this experiment are listed in Table \ref{tab:key_parameter_E1}.\\

An additional patch dataset has also been included, which tested the performance of anti-curriculum learning. For this dataset, the first stage only include road patches with a difficulty estimate $d(y, q)$ above 0.25.

\begin{table}[h]
\caption[Parameters for Experiment E1]{Key parameters for Experiment E1.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^p{11cm}}\hline
\rowstyle{\bfseries}
  - & parameters \\\hline
  Baseline & 120 epochs, s=220000, $d(y, q) < D_{\theta}$, $D_{0} = 1.00$, $D_{1} = 1.0$, $t_{start} = 50$  \\
  Curriculum & 120 epochs, s=220000, $d(y, q) < D_{\theta}$, $D_{0} = 0.25$, $D_{1} = 1.0$, $t_{start} = 50$ \\
  Anti-curriculum & 120 epochs, s=220000, $d(y, q) > D_{\theta}$, $D_{0} = 0.25$, $D_{1} = 0.0$, $t_{start} = 50$ \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E1}
\end{table}

\subsubsection{E2 - Curriculum Learning with Massachusetts Roads Dataset}
Experiment E2 had a similar setup to Experiment E1. There were two patch datasets, each with two stages, where the first patch dataset has been created by a curriculum strategy and the other by random sampling. In addition to comparing a baseline setup to a curriculum setup, various $D_{0}$ values for stage $0$ were tested. This will show how setting the threshold for the first stage will affect the performance of curriculum learning. Special parameters for this experiment can be found in Table \ref{tab:key_parameter_E2}.\\

\begin{table}[!h]
\caption[Parameters for Experiment E2]{Key parameters for Experiment E2.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^p{11cm}}\hline
\rowstyle{\bfseries}
  - & parameters \\\hline
  Baseline & 100 epochs, s=221600, $D_{0} = 1.0$,  $D_{1} = 1.0$, $t_{start} = 50$  \\
  Curriculum 0.15 & 100 epochs, s=221600, $D_{0} = 0.15$, $D_{1} = 1.0$, $t_{start} = 50$ \\
  Curriculum 0.25 & 100 epochs, s=221600, $D_{0} = 0.25$, $D_{1} = 1.0$, $t_{start} = 50$ \\
  Curriculum 0.35 & 100 epochs, s=221600, $D_{0} = 0.35$, $D_{1} = 1.0$, $t_{start} = 50$ \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E2}
\end{table}

The curriculum teacher for this experiment was trained with a patch dataset of 442800 examples, sampled from Massachusetts Roads Dataset. The teacher model was trained for 272 epoch. The model achieved a \ac{MSE} loss of 0.0225, and a relaxed precision and recall breakeven of 0.80 \todo{Relaxed precision only}.\\



\subsubsection{E3 - Bootstrapping with Massachusetts Roads Dataset}
The bootstrapping loss function was in this experiment compared to the cross-entropy loss function. The loss functions were compared by their performance on patch datasets with several levels of label degradation. This test introduced artificial omission noise, by removing roads from the label images. The network was trained for 140 epochs and with 110800 examples. The learning rate was slightly decreased compared to the default configuration. The bootstrapping loss function's $\beta$ parameter, was set at 1.0, and was incrementally decreased after epoch $M =90$, to $\beta_{min} = 0.9$, at a rate of \todo{Rate of what} The parameters relevant for this experiment are listed in Table \ref{tab:key_parameter_E3}.\\

\begin{table}[!ht]
\caption[Parameters for Experiment E3]{Key parameters for Experiment E3.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^p{11cm}}\hline
\rowstyle{\bfseries}
  - & parameters \\\hline
  Baseline & 100 epochs, s=110800, $a=0.0011$, $\mathcal{L}$ = cross-entropy, omission noise levels=0\%, 10\%, 20\%, 30\%, 40\%  \\
  Bootstrapping& 100 epochs, s=110800, $a=0.0011$, $\mathcal{L}$ = bootstrapping, $\beta_{max}$=1.0, $\beta_{min}$=0.9, $M$=60, omission noise levels=0\%, 10\%, 20\%, 30\%, 40\% \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E3}
\end{table}

\subsubsection{E4 - Bootstrapping with Norwegian Roads Dataset N50/VBase}
In contrast to experiment E3, this experiment tested the effect of bootstrapping for labels having a lot of registration noise. The training set consisted of examples from the Norwegian Roads Dataset N50. The label set N50 has coarser road center-line vectors, which results in a lot of registration error compared to the other aerial image datasets. The experiment optimized models with  $s = 165 000$ patch examples, for 140 epochs. The learning rate $a$ was slightly lower than the default. The bootstrapping parameter $\beta$ , was incrementally decreased from $\beta_{max}=1.0$ at epoch $M=90$, to $\beta_{min}=0.8$. Essentially, the bootstrapping loss functions incorporated it's own predictions starting from epoch 90, and then only at a rate of 0.2. \todo{Rewrite}\todo{Also problematic that decrease factor is not mentioned.} Any improvements should therefore be seen in the \ac{MSE} loss after this epoch. In addition, the confident bootstrapping loss function was also tested in this experiment. A summary of the experiment and key parameters can be found in Figure \ref{tab:key_parameter_E4}.\\

\begin{table}[h]
\caption[Parameters for E4]{Key parameters for E4.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^p{10cm}}\hline
\rowstyle{\bfseries}
  - & parameters \\\hline
  Baseline & 140 epochs, s=165000, $a=0.0011$, $\mathcal{L}$ = cross-entropy \\
  Bootstrapping&  140 epochs, s=165000, $a=0.0011$, $\mathcal{L}$ = bootstrapping, $\beta_{max}$=1.0, $\beta_{min}$=0.8, $M$=90\\
    Confident bootstrapping & 140 epochs, $a=0.0011$, s=165000, $\mathcal{L}$ = confident-bootstrapping, $\beta_{max}$=1.0, $\beta_{min}$=0.8, $M$=90\\
  \hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E4}
\end{table}

\subsubsection{E5 - Bootstrapping with Norwegian Roads Dataset VBase}
Experiment E5 tested the robustness towards label noise for the Norwegian Roads Dataset Vbase. This was done by comparing the performance of networks with different loss functions at several levels of omission noise. The experiment shared a very similar setup to Experiment E3. However, the parameter $\beta$ was decreased slightly more than E3. The experiment configuration is displayed in Table \ref{tab:key_parameter_E5}.\\

\begin{table}[h]
\caption[Parameters for Experiment E5]{Key parameters for Experiment E5.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^p{11cm}}\hline
\rowstyle{\bfseries}
  - & parameters \\\hline
  Baseline & 100 epochs, s=110000, $a=0.0011$, $\mathcal{L}$ = cross-entropy, omission noise levels=0\%, 10\%, 20\%, 30\%, 40\%  \\
  Bootstrapping&  100 epochs, s=110000, $a=0.0011$, $\mathcal{L}$ = bootstrapping, $\beta_{max}=1.0$, $\beta_{min}=0.8$, $M=60$, emission noise levels=0\%, 10\%, 20\%, 30\%, 40\% \\
    Confident bootstrapping & 100 epochs, s=110000, $a=0.0011$, $\mathcal{L}$ = confident-bootstrapping, $\beta_{max}=1.0$, $\beta_{min}=0.8$, $M=60$, omission noise levels=0\%, 10\%, 20\%, 30\%, 40\% \\
  \hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E5}
\end{table}

\subsubsection{E6 - Performance of the Road Detection System}
The performance of the road detection system was tested in Experiment E6. The patch dataset counts 3985200 examples, which have been randomly sampled from the Massachusetts Roads Dataset. The network was trained for 300 epochs, unless early stopping terminated the optimization. In order to train with such a large dataset, the model trained with only a subset of the examples at any given epoch. The training data was switched with another subset every 30th epoch starting from epoch 50 \todo{rewrite}. At any given epoch, the model was optimized by a total of 442800 examples. This is probably not as effective as using the entire dataset throughout training \todo{Check if this is correct}. Any discrepancies from the default network configuration are listed in Table \ref{tab:key_parameter_E6}.
\begin{table}[h]
\caption[Parameters for Experiment E6]{Key parameters for Experiment E6.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^p{11cm}}\hline
\rowstyle{\bfseries}
  - & parameters \\\hline
  Baseline & 300 epochs, $s=3 985 200$, $a=0.0015$, $b=128$, $\mathcal{L}$ = cross-entropy, initial\_patience = 400000  \\
  \hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E6}
\end{table}

\subsubsection{E7 - Curriculum Learning with an Inexperienced Teacher}
While experiment E1 and E2 had teachers that were trained with over 400000 examples, this  
experiment assumed that there are a limited amount of patches available. The teacher for this experiment was therefore only trained with 221600 examples, which is the same number of examples in each patch dataset. The model was trained for 156 epochs, and achieved a test \ac{MSE} of 0.0253 and a relaxed precision and recall of 0.79.\\

For the experiment involving gradually increasing the difficulty of the training set, there were 326800 examples in total, split between $N=5$ stages. The first stage had 110800 examples, whereas the remaining stages had 54000 examples each. For the baseline, the difficulty theshold $D_\theta$ was set to 1 for every stage. The first stage of the curriculum patch dataset, only allowed examples with a difficulty below 0.25. The key parameters for Experiment E7, are displayed in Table \ref{tab:key_parameter_E7}\\

\begin{table}[!h]
\caption[Parameters for Experiment E7]{Key parameters for Experiment E7.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^p{10cm}}\hline
\rowstyle{\bfseries}
  - & parameters \\\hline
  Baseline & 120 epochs, s=221600, $D_{0} = 1.0$,  $D_{1} = 1.0$, $t_{start} = 60$\\
  Curriculum & 120 epochs, s=221600, $D_{0} = 0.25$, $D_{1} = 1.0$, $t_{start} = 60$ \\
  Baseline - First stage only & 120 epochs, s=221600, $D_{0} = 1.0$\\
  Curriculum - First stage only & 120 epochs, s=221600, $D_{0} = 0.25$ \\
  Baseline - Gradual & 120 epochs, s=326800, $D_{\theta} = 1.0, \theta \in \{0, 1, 2, 3, 4\}$, $t_{start} = 60$,  $t_{stage} = 15$\\
  Curriculum - Gradual & 120 epochs, s=326800, $D_{0} = 0.25$, $D_{\theta} = 1.0, \theta \in \{1,2,3,4\}$ , $t_{start} = 60$,  $t_{stage} = 15$ \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E7}
\end{table}
