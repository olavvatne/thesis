\section{Experimental Setup}
\label{sec:experimentalSetup}
The network configuration used by most experiments is listed in Table \ref{tab:network_parameters}. Any deviations from this configuration will be detailed in this section. In addition, the specific hyperparameters used for each experiment can be found at \url{http://interface.ml/experiments}. Descriptions of tools used for conducting and presenting the experiments can be found in Appendix \ref{app:tools}. The relevant parameters for each experiment are detailed below.\\ 

\subsubsection{E1 - Bootstrapping with Massachusetts Roads Dataset}
In this experiment, the bootstrapping loss function was compared to the cross-entropy loss function. The loss functions were compared by their performance on patch datasets with several levels of label degradation. The network was trained for 140 epochs and with 110800 examples. The learning rate was slightly decreased compared to the default configuration. The bootstrapping loss function's $\beta$ parameter was set to 1.0, and was gradually decreased after epoch $M =90$, to $\beta_{min} = 0.9$, by multiplying $\beta$ with $\beta_{decrease}=0.9$ each epoch. The parameters relevant for this experiment are listed in Table \ref{tab:key_parameter_E1}.\\

Label noise has been artificially added to the label images before constructing the patch datasets. Specifically, areas of road pixels have been removed incrementally by setting the label pixel values to zero. This process is continued until a certain percentage of road class pixels have been removed. In the context of aerial imagery, the artificially added label noise simulates omission noise. This experiment utilized patch datasets where 0, 10, 20, 30, and 40 percent of roads were removed from each label image.\\ 

\begin{table}[h]
\caption[Parameters of Experiment E1]{Key parameters of Experiment E1.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+p{2.2cm} ^p{10cm}}\hline
\rowstyle{\bfseries}
  Method & Parameters \\\hline
  Baseline & 100 epochs, s=110800, $a=0.0011$, $\mathcal{L}$ = cross-entropy, omission noise levels=0\%, 10\%, 20\%, 30\%, 40\%  \\
  Bootstrapping& 100 epochs, s=110800, $a=0.0011$, $\mathcal{L}$ = bootstrapping, $\beta_{max}$=1.0, $\beta_{min}$=0.9, $M$=60, omission noise levels=0\%, 10\%, 20\%, 30\%, 40\% \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E1}
\end{table}

\subsubsection{E2 - Bootstrapping with Norwegian Roads Dataset VBase}
Experiment E2 tested the robustness towards label noise for the Norwegian Roads Dataset Vbase. This was done by comparing the performance of networks with different loss functions at several levels of omission noise. The experiment shared a very similar setup to Experiment E1. However, the parameter $\beta$ was decreased slightly more than in E1. The experiment configuration is displayed in Table \ref{tab:key_parameter_E2}.\\

\begin{table}[h]
\caption[Parameters of Experiment E2]{Key parameters of Experiment E2.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+p{2.2cm} ^p{10cm}}\hline
\rowstyle{\bfseries}
  Method & Parameters \\\hline
  Baseline & 100 epochs, s=110000, $a=0.0011$, $\mathcal{L}$ = cross-entropy, omission noise levels=0\%, 10\%, 20\%, 30\%, 40\%  \\
  Bootstrapping&  100 epochs, s=110000, $a=0.0011$, $\mathcal{L}$ = bootstrapping, $\beta_{max}=1.0$, $\beta_{min}=0.8$, $M=60$, emission noise levels=0\%, 10\%, 20\%, 30\%, 40\% \\
    Confident bootstrapping & 100 epochs, s=110000, $a=0.0011$, $\mathcal{L}$ = confident-bootstrapping, $\beta_{max}=1.0$, $\beta_{min}=0.8$, $M=60$, omission noise levels=0\%, 10\%, 20\%, 30\%, 40\% \\
  \hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E2}
\end{table}

\subsubsection{E3 - Bootstrapping with Norwegian Roads Dataset N50/VBase}
In contrast to Experiment E2, this experiment tested the effect of bootstrapping for \todo{to/of?} labels having a lot of registration noise. The training set consisted of examples from the Norwegian Roads Dataset N50. The label set N50 has coarser road centerline vectors, which result in a lot of registration noise compared to the other aerial image datasets. The experiment optimized models with  $s = 165 000$ patch examples, for 140 epochs. The learning rate $a$ was slightly lower than the default. The bootstrapping parameter $\beta$ was gradually decreased from $\beta_{max}=1.0$ at epoch $M=90$, to $\beta_{min}=0.8$. Essentially, the bootstrapping loss functions incorporated its own predictions starting from epoch 90. Any improvements should, therefore, be seen in the test loss after this epoch. In addition, the confident bootstrapping loss function was also tested in this experiment. A summary of the experiment and key parameters can be found in Table \ref{tab:key_parameter_E3}.\\

\begin{table}[H]
\caption[Parameters of E3]{Key parameters of E3.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+p{2.2cm} ^p{10cm}}\hline
\rowstyle{\bfseries}
  Method & Parameters \\\hline
  Baseline & 140 epochs, s=165000, $a=0.0011$, $\mathcal{L}$ = cross-entropy \\
  Bootstrapping&  140 epochs, s=165000, $a=0.0011$, $\mathcal{L}$ = bootstrapping, $\beta_{max}$=1.0, $\beta_{min}$=0.8, $M$=90\\
    Confident bootstrapping & 140 epochs, s=165000, $a=0.0011$, $\mathcal{L}$ = confident-bootstrapping, $\beta_{max}$=1.0, $\beta_{min}$=0.8, $M$=90\\
  \hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E3}
\end{table}

\subsubsection{E4 - Curriculum Learning with Massachusetts Roads Dataset}
This experiment involved measuring the performance between a baseline and  a curriculum patch dataset generated from the Massachusetts Roads Dataset. Both datasets have $N=2$ stages, where each stage includes 110800 training examples. Each stage $\theta$ contains examples where the difficulty estimate $d(y, q)$ is less than a threshold $D_\theta$. The baseline patch dataset has a threshold $D_\theta$ of 1.0 for both stages, which constitutes random sampling. When switching between the first and second stage the entire training set is replaced by examples from the second stage. This occurred at epoch $t_{start}=50$. An additional patch dataset has also been included, which tested the performance of anti-curriculum learning. In this dataset, the first stage excluded patches containing road pixels with a difficulty estimate $d(y, q)$ less than 0.25. Special parameters for this experiment can be found in Table \ref{tab:key_parameter_E4}.\\

\begin{table}[h]
\caption[Parameters of Experiment E4]{Key parameters of Experiment E4.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{++p{2.5cm} ^p{10cm}}\hline
\rowstyle{\bfseries}
  Method & Parameters \\\hline
  Baseline & 120 epochs, s=220000, $d(y, q) < D_{\theta}$, $D_{0} = 1.00$, $D_{1} = 1.0$, $t_{start} = 50$  \\
  Curriculum & 120 epochs, s=220000, $d(y, q) < D_{\theta}$, $D_{0} = 0.25$, $D_{1} = 1.0$, $t_{start} = 50$ \\
  Anti-curriculum & 120 epochs, s=220000, $d(y, q) > D_{\theta}$, $D_{0} = 0.25$, $D_{1} = 0.0$, $t_{start} = 50$ \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E4}
\end{table}

The experiment's \todo{better than of after?} curriculum teacher was trained with a patch dataset of 442800 examples, sampled from Massachusetts Roads Dataset. The teacher classifier was trained for 272 epochs. It achieved a \ac{MSE} test loss of 0.0225, and a relaxed precision and recall breakeven point of 0.79.\\

\subsubsection{E5 - Curriculum Learning with Norwegian Roads Dataset}
Experiment E5 had a similar setup to Experiment E4. There were four different patch datasets, each with two stages. The baseline patch dataset was created by random sampling, whereas the others were created by a curriculum strategy. Three different curriculum patch datasets were tested, where each stage $0$ has a different $D_{0}$ threshold value.\\

The curriculum teacher that generated predictions for the difficulty estimation was a previously trained model. The teacher classifier was trained with a dataset consisting of 440000 examples for \todo{må sjekke, spurt Ollie} 174 epochs. The learning rate was multiplied by 0.85 every 50th epoch. Apart from that, the network parameters closely resembles the default parameters listed in Table \ref{tab:network_parameters}. The classifier's final \ac{MSE} test loss was 0.0222, and the relaxed precision and recall breakeven point was around 0.71. \\

The default network configuration was used for \todo{Må skjekke, spurt Ollie} the networks trained on the patch datasets. Essentially, the only real difference between them was the first stage of the datasets. The models were trained for 120 epochs with a stage switch at epoch 50. Important parameters for \todo{of?} this experiment are listed in Table \ref{tab:key_parameter_E5}.\\

\begin{table}[h]
\caption[Parameters of Experiment E5]{Key parameters of Experiment E5.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+p{2.5cm} ^p{9.5cm}}\hline
\rowstyle{\bfseries}
  Method & Parameters \\\hline
  Baseline & 100 epochs, s=221600, $D_{0} = 1.0$,  $D_{1} = 1.0$, $t_{start} = 50$  \\
  Curriculum 0.15 & 100 epochs, s=221600, $D_{0} = 0.15$, $D_{1} = 1.0$, $t_{start} = 50$ \\
  Curriculum 0.25 & 100 epochs, s=221600, $D_{0} = 0.25$, $D_{1} = 1.0$, $t_{start} = 50$ \\
  Curriculum 0.35 & 100 epochs, s=221600, $D_{0} = 0.35$, $D_{1} = 1.0$, $t_{start} = 50$ \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E5}
\end{table}

\subsubsection{E6 - Curriculum Learning with an Inexperienced Teacher}
While Experiment E4 and E5 had teachers that were trained with over 400000 examples, this  
experiment assumed that there is a limited amount of patches available. The teacher of this experiment was therefore only trained with 221600 examples, which is the same number of examples in each patch dataset. The model was trained for 156 epochs, and achieved a \ac{MSE} test loss of 0.0253, and a relaxed precision and recall breakeven point of 0.79.\\

In the experiment involving gradually increasing the difficulty of the training set, there were 326800 examples in total, split between $N=5$ stages. The first stage had 110800 examples, whereas the remaining stages had 54000 examples each. In the baseline, the difficulty threshold $D_\theta$ was set to 1 for every stage. The first stage of the curriculum patch dataset only allowed examples with a difficulty below 0.25. The key parameters of Experiment E6, are displayed in Table \ref{tab:key_parameter_E6}\\

\begin{table}[p]
\caption[Parameters of Experiment E6]{Key parameters of Experiment E6.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+p{2.5cm} ^p{9.5cm}}\hline
\rowstyle{\bfseries}
  Method & Parameters \\\hline
  Baseline & 120 epochs, s=221600, $D_{0} = 1.0$,  $D_{1} = 1.0$, $t_{start} = 60$\\
  Curriculum & 120 epochs, s=221600, $D_{0} = 0.25$, $D_{1} = 1.0$, $t_{start} = 60$ \\
  Baseline, First stage only & 120 epochs, s=221600, $D_{0} = 1.0$\\
  Curriculum, First stage only & 120 epochs, s=221600, $D_{0} = 0.25$ \\
  Baseline, Gradual & 120 epochs, s=326800, $D_{\theta} = 1.0, \theta \in \{0, 1, 2, 3, 4\}$, $t_{start} = 60$,  $t_{stage} = 15$\\
  Curriculum, Gradual & 120 epochs, s=326800, $D_{0} = 0.25$, $D_{\theta} = 1.0, \theta \in \{1,2,3,4\}$ , $t_{start} = 60$,  $t_{stage} = 15$ \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E6}
\end{table}

\subsubsection{E7 - Performance of the Road Detection System}
The performance of the road detection system was tested in Experiment E7. The M1 network utilized a patch dataset with 3985200 examples that were randomly sampled from the Massachusetts Roads Dataset. The network was trained for 300 epochs, unless early stopping terminated the optimization at an earlier point. In order to train with such a large dataset, the network trained with only a subset of the examples at any given epoch. The training data was switched with another subset at every $epoch\mod30=0$, starting from epoch 50. At any given epoch, the network was optimized by a total of 442800 examples. Any discrepancies from the default network configuration are listed in Table \ref{tab:key_parameter_E7}.\\

In addition to the M1 network, Network M2 and N1 further tested the road detection performance on both aerial imagery datasets. The default network architecture was altered in both networks, as seen in Table  \ref{tab:key_parameter_E7}. This resulted in a significant increase in adjustable parameters compared to M1. Both networks utilized the {\it confident bootstrapping} loss function, and were trained by curriculum patch datasets. The curriculum teachers of Experiment E4 and E5 were used to create the patch datasets.\\

\begin{table}[p]
\caption[Parameters of Experiment E7]{Key parameters of Experiment E7.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+p{2.4cm} ^p{9.8cm}}\hline
\rowstyle{\bfseries}
  Method & Parameters \\\hline
  M1-Massachusetts, cross-entropy, no-curriculum & 300 epochs, $s=3985200$, $a=0.0015$, $b=128$, $\mathcal{L}$ = cross-entropy, $ES_{init}$ = 400000  \\
  M2-Massachusetts, bootstrapping, curriculum & 85 epochs, $s=1772800$, $a=0.0025$,$a_{decrease}=0.9$ $b=128$, $m=0.93$, $\mathcal{L}$ = confident bootstrapping, $ES_{init}$ = 500000, $K^{(0)}=120$, $CLF^{(0)}=(16,16)$, $CLF^{(1)}=(8,8)$, $CLS^{(0)}=(2,2)$, $p^{0,1,2}=0.85$, $D_{0} = 0.25$, $D_{\theta} = 1.0, \theta \in \{1,2,3,4,5,6\}$ , $t_{start} = 30$,  $t_{stage} = 15$  \\
    N3 - Norway, bootstrapping, curriculum & 85 epochs, $s=1760000$, $a=0.0025$, $a_{decrease}=0.9$, $b=128$, $m=0.93$ $\mathcal{L}$ = confident bootstrapping, $ES_{init}$ = 500000, $K^{(0)}=120$, $CLF^{(0)}=(16,16)$, $CLF^{(1)}=(8,8)$, $CLS^{(0)}=(2,2)$, $p^{0,1,2}=0.85$, $D_{0} = 0.25$, $D_{\theta} = 1.0, \theta \in \{1,2,3,4,5,6\}$ , $t_{start} = 30$,  $t_{stage} = 15$  \\
  \hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E7}
\end{table}
