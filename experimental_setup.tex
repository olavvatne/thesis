\section{Experimental Setup}
\label{sec:experimentalSetup}
The network configuration for most experiments are listed in Table \ref{tab:network_parameters}. Any deviations from this configuration will be detailed in this section. In addition, the specific hyperparameters used for each experiment can be found at \url{http://interface.ml/experiments}. Descriptions about tools used for conducting the experiments can be found in Appendix \ref{app:tools}, and the relevant parameters for each experiment are detailed below.\\ 

\subsubsection{E1 - Curriculum learning with Norwegian Roads Dataset}
This experiment involves comparing two patch datasets generated from the Norwegian Roads Dataset. Both datasets have $S=2$ stages, where each stage include 110 000 training examples. The first patch dataset is created according to a curriculum strategy, where the difficulty estimate $d(y, q)$ is less than a threshold $D_\theta$. The first stage consists of only examples with a difficulty estimate below 0.25,  while the second stage have a threshold of 1.0, which is equivalent to random sampling of patches. The second patch dataset have a threshold $D_\theta$ of 1.0 for both stages, and constitute the baseline. Switching between the first and second stage, happens by entirely replacing the training set by sampling without replacement \todo{Need to know if sampling without replacement is correct term}.\\

The curriculum teacher, which create predictions for the difficulty estimation of examples, are a previous model. The teacher classifier was trained with a dataset consisting of 440000 examples for 175 epochs. Otherwise, the network parameters closely resembles the default parameters listed in Table \ref{tab:network_parameters} \todo{True?}. The classifier's final \ac{MSE} test loss is 0.0222, and the precision and recall breakeven is around 0.66. \\

The default network configuration is used for models trained on both the curriculum dataset and the baseline dataset. Essentially, the only real difference between the two, is the first stage of the datasets. The models are trained for 120 epochs, with the stage switch after 50 epochs. Important parameters for this experiment are listed in Table \ref{tab:key_parameter_E1}.\\

An additional patch dataset has also been included, which tests the performance of anti-curriculum learning. For this dataset, the first stage only include road patches with a difficulty estimate $d(y, q)$ above 0.25.

\begin{table}[h]
\caption{Key parameters for E1.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^p{11cm}}\hline
\rowstyle{\bfseries}
  - & parameters \\\hline
  Baseline & 120 epochs, s=110000, $d(y, q) < D_{\theta}$, $D_{0} = 1.00$, $D_{1} = 1.0$, $t_{start} = 50$  \\
  Curriculum & 120 epochs, s=110000, $d(y, q) < D_{\theta}$, $D_{0} = 0.25$, $D_{1} = 1.0$, $t_{start} = 50$ \\
  Anti-curriculum & 120 epochs, s=110000, $d(y, q) > D_{\theta}$, $D_{0} = 0.25$, $D_{1} = 0.0$, $t_{start} = 50$ \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E1}
\end{table}

\subsubsection{E2 - Curriculum learning with Massachusetts Roads Dataset}
Experiment E2 have a similar setup to Experiment E1. patch datasets with two stages, where the first patch dataset has been created by a curriculum strategy and the other by random sampling. In addition to comparing a baseline setup to a curriculum setup, different $D_{0}$ values for stage $0$ have been tested. This will show how setting the threshold for the first stage will affect the performance of curriculum learning. Special parameters for this experiment can be found in Table \ref{tab:key_parameter_E2}.\\

\begin{table}[!h]
\caption{Key parameters for E2.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^p{11cm}}\hline
\rowstyle{\bfseries}
  - & parameters \\\hline
  Baseline & 100 epochs, s=110800, $D_{0} = 1.0$,  $D_{1} = 1.0$, $t_{start} = 50$  \\
  Curriculum 0.15 & 100 epochs, s=110800, $D_{0} = 0.15$, $D_{1} = 1.0$, $t_{start} = 50$ \\
  Curriculum 0.25 & 100 epochs, s=110800, $D_{0} = 0.25$, $D_{1} = 1.0$, $t_{start} = 50$ \\
  Curriculum 0.35 & 100 epochs, s=110800, $D_{0} = 0.35$, $D_{1} = 1.0$, $t_{start} = 50$ \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E2}
\end{table}

The curriculum teacher for this experiment was trained with a patch dataset of 442800 examples, sampled from Massachusetts Roads Dataset. The teacher model was trained for 272 epoch. The model achieved a \ac{MSE} loss of 0.0225, and a precision and recall breakeven of 0.76 \todo{Relaxed precision only}.\\



\subsubsection{E3 - Bootstrapping with Massachusetts Roads Dataset}
The bootstrapping loss function is in this experiment compared to the cross-entropy loss function. The loss functions are compared by their performance on patch datasets with several levels of label degradation. This test introduce artificial omission noise, by removing roads from the label images. The network are trained for 140 epochs and 110 800 examples. The learning rate is slightly decreased compared to the default configuration. The bootstrapping loss function's $\beta$ parameter, are set at 1.0, and are incrementally decreased after epoch $M =90$, to $\beta_{min} = 0.9$ \todo{What happens after epoch 90, will reveal stuff}. The parameters relevant for this experiment are listed in Table \ref{tab:key_parameter_E3}.\\

\begin{table}[!ht]
\caption{Key parameters for E3.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^p{11cm}}\hline
\rowstyle{\bfseries}
  - & parameters \\\hline
  Baseline & 100 epochs, s=110800, $a=0.0011$, $\mathcal{L}$ = cross-entropy, emission noise levels=0\%, 10\%, 20\%, 30\%, 40\%  \\
  Bootstrapping& 100 epochs, s=110800, $a=0.0011$, $\mathcal{L}$ = bootstrapping, $\beta_{max}$=1.0, $\beta_{min}$=0.9, $M$=60, emission noise levels=0\%, 10\%, 20\%, 30\%, 40\% \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E3}
\end{table}

\subsubsection{E4 - Bootstrapping with Norwegian Roads Dataset N50/VBase}
In contrast to experiment E3, this experiment tests the effect of bootstrapping for labels having a lot of registration noise. The training set consists of examples from the Norwegian Roads Dataset N50. The label set N50, has coarser road center-line vectors, which results in a lot of registration error, compared to the other datasets. The experiment optimize models with  $s = 165 000$ patch examples, for 140 epochs. The learning rate $a$ is slightly lower. The $\beta$ bootstrapping parameter, is incrementally decreased from $\beta_{max}=1.0$ at epoch $M=90$, to $\beta_{min}=0.8$. Essentially, the bootstrapping loss functions incorporate it's own predictions starting from epoch 90, and then only at a rate of 0.2. \todo{Rewrite}\todo{Also problematic that decrease factor is not mentioned.} Any improvements should therefore be seen in the \ac{MSE} loss after epoch $M$. In addition, the confident bootstrapping loss function is also tested in this experiment. A summary of the experiment and key parameters can be found in figure \ref{tab:key_parameter_E4}\\

\begin{table}[h]
\caption{Key parameters for E4.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^p{10cm}}\hline
\rowstyle{\bfseries}
  - & parameters \\\hline
  Baseline & 140 epochs, s=165000, $a=0.0011$, $\mathcal{L}$ = cross-entropy \\
  Bootstrapping&  140 epochs, s=165000, $a=0.0011$, $\mathcal{L}$ = bootstrapping, $\beta_{max}$=1.0, $\beta_{min}$=0.8, $M$=90\\
    Confident bootstrapping & 140 epochs, $a=0.0011$, s=165000, $\mathcal{L}$ = confident-bootstrapping, $\beta_{max}$=1.0, $\beta_{min}$=0.8, $M$=90\\
  \hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E4}
\end{table}

\subsubsection{E5 - Bootstrapping with Norwegian Roads Dataset VBase}
Experiment E5 tests the robustness towards label noise for the Norwegian Roads Dataset Vbase. This is done by comparing the performance of models with different loss functions at several levels of omission noise, which are artificially added to the labels. The experiment shares a very similar setup to Experiment E3. However, the parameter $\beta$ is decreased slightly more than E3. The experiment configuration is displayed in Table \ref{tab:key_parameter_E5}.\\

\begin{table}[h]
\caption{Key parameters for E5.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^p{11cm}}\hline
\rowstyle{\bfseries}
  - & parameters \\\hline
  Baseline & 100 epochs, s=110000, $a=0.0011$, $\mathcal{L}$ = cross-entropy, emission noise levels=0\%, 10\%, 20\%, 30\%, 40\%  \\
  Bootstrapping&  100 epochs, s=110000, $a=0.0011$, $\mathcal{L}$ = bootstrapping, $\beta_{max}=1.0$, $\beta_{min}=0.8$, $M=60$, emission noise levels=0\%, 10\%, 20\%, 30\%, 40\% \\
    Confident bootstrapping & 100 epochs, s=110000, $a=0.0011$, $\mathcal{L}$ = confident-bootstrapping, $\beta_{max}=1.0$, $\beta_{min}=0.8$, $M=60$, emission noise levels=0\%, 10\%, 20\%, 30\%, 40\% \\
  \hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E5}
\end{table}

\subsubsection{E6 - Performance of road detection system}
The performance of the road detection system is tested in Experiment E6. The patch dataset counts 3985200 examples, which have been randomly sampled from the Massachusetts Roads Dataset. The network is trained for 300 epochs, unless early stopping terminates the optimization before this. In order to train with such a big dataset, the model trains with only a subset of the examples at any given epoch. The training data is switched with another subset every 30th epoch starting from epoch 50 \todo{rewrite}. At any given epoch, the model is optimized by a total of 442800 examples. This is probably not as effective as using the entire dataset throughout training \todo{Check if this is correct}. Any discrepancies from the default network configuration are listed in Table \ref{tab:key_parameter_E6}.
\begin{table}[h]
\caption{Key parameters for E6.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^p{11cm}}\hline
\rowstyle{\bfseries}
  - & parameters \\\hline
  Baseline & 300 epochs, $s=3 985 200$, $a=0.0015$, $b=128$, $\mathcal{L}$ = cross-entropy, initial\_patience = 400000  \\
  \hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:key_parameter_E6}
\end{table}