\section{Semantic Segmentation with Convolutional Neural Network}
\label{sec:network}
\todo[inline]{Draft edition. Create revision of section}
This section will present the convolutional neural network in detail. \todo[inline]{Better and a slightly longer introduction to section}

\subsection{Patch-based architecture}
\todo[inline]{Explain what this actually does!}
\todo[inline]{Outline what this thing actually predict. Patch based approach}
\todo[inline]{Input bigger than output. Context, etc}
\todo[inline]{Explain layering etc. show image}
\todo[inline]{Have to justify model layering. 256 output? why 64x64 input images why? Context. }
\todo[inline]{Leaky ReLu}
\todo[inline]{Can this be placed here?}

The system is based on the deep neural network outlined by \cite{Mnih_aerial_images_noisy}. The network have three convolutional layers and two fully connected layers. This network architecture is depicted in Figure \ref{fig:conv}. After the network is trained, it can predict whether or not roads are present in a $16 \times 16$ pixel area contained in the center of a $64 \times 64$ aerial image patch. The input patch is considerably larger than the output patch, so that the network can better utilize the context in the image. \\

The first layer perform convolution using $13 \times 13$ kernels, and outputs in total 64 feature maps. Only the first layer utilize max pooling, which reduces the number of inputs to the next layer as well as introducing some translational invariance to the model. The kernel size in the second and third layer are currently $4 \times 4$ and $3 \times 3$, respectably. The output of the third convolutional layer is used as input to a fully connected neural network with a single hidden layer and an output layer. The latter contains 256 units where each output is the probability of a pixel representing a road.\\

\begin{figure}
\begin{center}
\includegraphics[width=1\columnwidth]{figs/network/Filter_unblurred.png}
\caption[Visualization of filter map]{Visualization of filter maps from the first convolutional layer of a trained network.}
\label{fig:convoluional_first_layer_visualization}
\end{center}
\end{figure}

\begin{table}[htp]
\caption{Hyperparameters for \ac{CNN}}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^l ^l}\hline
\rowstyle{\bfseries}
  Parameter & Description & Value\\\hline
  $K_i$ & Number of kernels for convolutional layer i & 64, 112, 80 \\
  $CLF_i$ & Filter size of convolutional layer $i$ & (13,13),(4,4),(3,3) \\
  $CLS_i$ & Stride amount of convolutional layer $i$ & (4,4),(1,1),(1,1) \\
  $CLP_i$ & Pool size of convolutional layer $i$ & (2,2),(1,1),(1,1) \\
  Cost function & Function that is minimzed each iteration & Cross entropy \\
  Epochs & How many epochs \ac{CNN} will train for. & 100 \\
  $\theta_i$ & Transfer function for each value & Leaky ReLU x4, sigmoid x1 \\
  $h$ & Number of neurons in hidden layer. & 4096 \\
  $p_i$ & Dropout rate for a layer $i$ & 1.0, 0.9, 0.8, 0.5, 1.0 \\
  $a$ & Learning rate & 0.0014 \\
  $a_{decrease}$ & learning rate decrease factor & 0.95 \\
  $\lambda$ & L2 Weight decay strength & 0.0001 \\
  $m$ & Momentum & 0.9 \\
  $b$ & Batch size & 64 \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:curriculum_parameters}
\end{table}

\subsection{Optimization}
%\url{http://cs231n.github.io/neural-networks-3/}
%http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}
%http://arxiv.org/pdf/1212.0901v2.pdf
%http://stats.stackexchange.com/questions/179915/whats-the-difference-between-momentum-based-gradient-descent-and-nesterovs-ac
The model parameters are optimized with a special form of \ac{SGD}, called Nesterov momentum stochastic gradient decent, and have an improved stability and faster convergence compared to standard \ac{SGD} \citep{Bengio_advances_optimizing}. The standard momentum approach extend the gradient update step, by introducing velocity. The loss can be interpreted as a hilly error surface where loss optimization can be viewed as a particle gaining and losing velocity while interacting with the landscape. When the loss optimization is gaining velocity, it will stop doing steepest decent, and results in a smoother decent with less oscillations. Basically, instead of directly using the gradient to move in the landscape, the gradient will influence the velocity. The only hyperparameter associated with momentum is $m$, which damps the velocity, and can be interpreted as friction. \\


Nesterov momentum is a variation of momentum. Unlike standard momentum, the Nesterov method first makes a step in the direction of the accumulated velocity, and then makes a correction of the velocity based on the new location. In standard momentum a correction to the velocity is made before taking a step. The Nesterov gradient is defined by the following update rule:

\begin{flalign*}
     &v_{t} = mv_{t-1} - a \nabla f(\theta_{t-1} + m v_{t-1}) \\
     &\theta_t = \theta_{t-1} + v_t,
\end{flalign*}

\noindent where $\theta_t$ is the model parameters, $v_t$ the velocity, $m$ the momentum decay, $a$ the current learning rate and $\nabla f(\theta)$ is the gradient.\\

Furthermore, the system also support RMSProp which keeps a running average of the gradient magnitude for every weight. This is used to adaptively adjust the learning rate of each weight. Compared to regular \ac{SGD}, this will result in a faster convergence. \todo{Paper detailing RMSprop}\\ 

All the above mentioned optimization approaches have an important hyperparameter in common, and that is the learning rate $a$. In this implementation the learning rate is annealed over time, by step decay. The learning rate is decreased in regular intervals of epochs by some factor $a_{decrease}$ during optimization. \\

\todo[inline]{Learning and negative log likelihood?. Optimization of the loss. So the loss is negative log likelihood}
%http://neuralnetworksanddeeplearning.com/chap3.html crossentropy!!
\subsection{Regularization}
To avoid overfitting the training data, and hopefully achieve better generalization, different regularization schemes are applied during optimization, such as L2 weight decay, early stopping, and dropout. The model's task is to learn the regularities found in the mappings between the data and labels in the training set. However, the training set also contain sampling errors and accidental patterns not relevant to the task at hand. Without regularization, a model will learns the useful, but also the erroneous regularities a bit to well, and can therefore start to overfit the data. \\

L2 regularization is a weight decay method which applies a loss function penalty to prevent weights in the network from growing large. This is achieved by adding an extra term, $\lambda\sum_{i=0}^{|w|} w_i^2$ to the loss function, which penalize large valued weights, and encourages a smoother parameter configuration \citep{Hinton_regularization}. The strength of the weight decay is controlled by the $\lambda$ hyperparameter, which is typically is set to a low value \todo{Ok to cite lecture note???}.\\

Early stopping stops the optimization process when performance on the validation set starts to consistently decrease. When the validation loss stops or starts to increase in value, the method waits for a certain number of iterations before stops the training. There are three parameters that controls the early stopping behaviour, \textit{initial patience}, \textit{improvement threshold} and \textit{patience increase}. The first parameter controls how many iterations the training should run for regardless of validation loss improvements. The second parameter dictates the required amount of improvement of the current validation loss compared to the best previously seen validation loss, before incrementing the patience. The latter parameter decide the increment factor of the patience. The patience, or how many additional iterations early stopping will wait for an validation loss improvement is defined as $\textit{patience} = max(\textit{patience}, \textit{iteration} \times \textit{patience increase} )$ \todo{Paper, and is formula really necessary???}\\
%http://link.springer.com/chapter/10.1007/3-540-49430-8_3

The dropout method forces the units to rely less on each other, by randomly disabling units in the network during training. This encourage units to encode independently useful information, since dropout penalize co-adaptation between units \citep{Srivastava_dropout}. Dropout can also be viewed as an ensemble method approximation. By randomly removing subsets of units from the network, an exponential number of parameter sharing subnetworks are trained simultaneously. This is less computationally expensive than training an ensemble of many individual neural networks. At test time, the predictions of these subnetworks are combined by an approximate averaging method \todo{Bad way to say this}. This is done by having the neural network creat predictions without dropping any units. Because all units are used, the weights of network have to be scaled down. If a unit is retained with a probability of $p$ during training, the outgoing weights of that unit should be multiplied by $p$ at test time. Each layer $i$ has an unique dropout rate parameter, $p_i$, which controls the probability of dropping an unit at that layer.  In the \ac{CNN} implementation, dropout has been implemented with some minor tweaks: \\

\todo{VERY BAD}
\begin{flalign*}
     &Y = \frac{1}{p^{(l)}} R \sigma(W^{(l)}X + B^{(l)}),
\end{flalign*}

\noindent where $W$ and $B$ are the weights and biases in layer $l$, $X$ denote the input potentials and f(z) is the transfer function. $R$ is a vector of independent Bernoulli random variables each having a probability of $p^{l}$ being 1. Instead of multiplying the outcoming weights at test time, the output potentials during training are divided by $p^{(l)}$, which should give essentially the same scaling effect.\\

\subsection{Data preprocessing}
The datasets used in this thesis contains aerial images and road label maps that are very large. Each image is typically around $1500 \times 1500$ pixels in size, which is arguably to large for the model to training with. Smaller patches of examples are therefore extracted from these larger images. From each image a certain number of data patches and label patches of $64 \times 64$ and $16 \times 16$ are extracted randomly and added to a patch dataset.\\

However, before extracting patches, each aerial image and road label map is rotated by a random number of degrees between 0 and 360 degrees. This is necessary because of orientation bias in the datasets \todo{DO they?}. In many cities the road network is often constructed in a grid pattern, which leads to roads having certain orientations. Without random rotations of the training data, the model might become better at detecting roads only at certain orientations \citep{Mnih_roads_high_res_aerial_images}. Each image is therefore rotated by random angles before extracting patches, which results in a patch training set that encourages the model to learn invariance to orientation.\\

Another data augmentation method utilized when constructing the patch training set is flipping. This is a common method for artificially increase the size of the dataset \citep{Krizhevsky_imagenet}. Since the dataset contains aerial imagery that depict landscape from a top-down perspective, the patches can both be flipped vertically and horizontally \todo{Something missing here}.\\

Contrast normalization is also applied to every patch. each patch is normalized by subtracting the mean pixel value from the pixels of a patch, and then dividing the result by the standard deviation found for all pixels in the dataset.\\

Additionally, the datasets used exhibit some class imbalance \citep{Japkowicz_class_imbalance}, which can be problematic since the model might decide to prioritize loss minimization of the high-occurence classes and ignore low-occurring classes. Random sampling of the datasets, lead to a large imbalance between road examples and non-road examples. The proportion of patches containing road according to the labelling, range between 8\% and 25\%\todo{Check numbers}, depending on the dataset. This is solved by simply artificially increasing the proportion of road examples, when sampling the patch dataset. This method is not applied to the test and validation patch set, and will therefore retain their natural class distribution\todo{Improve language}.\\