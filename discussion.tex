\section{Discussion}\todo{Can be called analysis}
\label{sec:Discussion}
In the following discussion, the results presented in the previous section will be analysed more in-depth. Furthermore, qualitative results from the road detection system are evaluated, and might illustrate why methods for dealing with noisy labels are important for this type of dataset.\\

The \ac{CNN} and the proposed methods are in some sense comparable to the three groups of approaches for dealing with label noise, described in \ref{sec:background_label_noise}. The bootstrapping loss function is clearly a noise-tolerant approach, where the loss function is modified in an attempt to make the network more robust towards label noise. The \ac{CNN} and the regularization methods used, can be categorized as a noise-robust model. Whereas, curriculum learning in some sense can be described as a data cleansing method. The training set does not exclude any examples from all stages, or relabel examples. But, the "simple" stages are created based on filtering techniques, where inconsistencies between label and prediction determine whether an example is excluded from a training set stage.  \\


\subsection{The Effect of Bootstrapping}
Unfortunately, the effect of employing the bootstrapping loss function is quite small. However, the bootstrapping methods did perform nearly equal or slightly better when looking at the breakeven values in Experiment E3, E4 and E5. And for increasing levels of omission noise,  as seen in Figure \ref{fig:E5_boot_norway}, and Figure \ref{fig:E5_boot_norway}, it seems that bootstrapping do exhibit some robustness towards label noise. Compared to the performance of the baseline, the difference in both test loss and breakeven seems to be increasing.  \\

Even though up to 40\% of the roads present in the label images was removed, it did not particularly affect the baseline much. The baseline network seems to be surprisingly robust towards label noise. However, the default patch dataset sampling policy might be somewhat responsible for this.\\

In Experiment E3 and E5, only omission noise was artificially added to the label images. This simply removed road pixels from the label images, until a certain percentage of road pixels had been removed. However, when the patch creator sampled the aerial dataset for example patches, the preference for an even balance between patches containing road pixels and patches not containing any road pixels, was enabled. Even for increasing levels of omission noise, the patch creator still sampled around 50\% road patches with almost no noise added. The non-road patches of the dataset were of course affected by the increase in label noise \todo{Should add artificial registration noise. Hard to add realistic registration noise}.\\

There is also the issue of seemingly contradictory results in Experiment E4. Even though bootstrapping in Figure \ref{fig:E4_boot_norway_vbase} shows an increase in the loss towards the end, it still achieves a better precision and recall curve than cross-entropy loss. This is probably an indication of bootstrapping actually working. It seems that the bootstrapping loss function slightly adjust the predictions to be more consistent between perceptually similar examples, at the cost of an increasing test loss. The Vbase test set labels are not perfect, and exhibit some registration and omission noise, which probably explains the increase in test loss. The bootstrapping function might be reducing the impact of local registration noise,  adjusting the predictions to fit the road pixels better. These prediction adjustments will be very visible in the MSE loss, but will not affect the precision and recall because of the relaxed measure of precision \todo{Should find a image example of this!, And already mentioned in 4.3.2}. \\

The alternative bootstrapping loss function  behaves slightly different compared to bootstrapping, as demonstrated by the test loss figures, \ref{fig:E5_boot_norway_loss} and \ref{fig:E4_boot_norway_vbase_loss}. In these figures, the confident bootstrapping seems to follow the general outline of the baseline, more closely than bootstrapping. The only difference between the two, is that the confident bootstrapping loss function modifies targets using only the confident pixel predictions.  In addition, For the N50 label set, this loss function actually performed slightly better than bootstrapping and the baseline \todo{Oh my glob}.\\ 

In summary, the experiments show that the bootstrapping can successfully utilize the implicit knowledge stored in the network. But, the possible performance gain might not justify its use. \\

%An additional challenge was the constraint on runtime. To run 10 replicate experiments, the patch dataset size had to be limited. This might impact the performance of the bootstrapping methods, since they rely on models that have already incorporated some implicit knowledge about the data.\\

\subsection{Curriculum Learning by Utilizing an Artificial Teacher}
All experiments comparing a randomly sampled patch dataset and a patch dataset constructed from a curriculum strategy, demonstrates that presenting less difficulty examples first have a positive impact on the generalization ability of the classifier \todo{Generalization ability?}. Furthermore, the proposed curriculum strategy, based on measuring disagreement between the labels and the predictions of a teacher classifier, seems to be a viable approach for conducting curriculum learning. \\


Experiment E1 and E2 demonstrated that the examples presented first do impact the final performance of the network. This is evident from both the precision and recall curve as well as the MSE loss. It is conceivable that the a less challenging training set distribution, puts the network in an advantages area of parameter space. Even though the latter half of training for the curriculum tests were conducted on a training set with the same example distribution as the baseline tests, the starting advantage of curriculum learning was still preserved in the final performance.  \\

From Experiment E2, it is also clear that anti-curriculum learning does not provide the same advantage as curriculum learning. The performance in Figure \ref{fig:E2_curr_mass_loss} converges already at around epoch 25, with a test loss considerably higher than the baseline. It is only able to approach the test loss of the baseline after switching to the natural example distribution at epoch 50. The final test loss converged to a level well above the baseline. This also illustrate that the examples presented first, have a bigger influence on the outcome, than examples presented later on. It is possible that early optimization on harder examples, guides the network to a unfavourable local minima, which is hard to escape from later on.\\

The results further shows that the outcome of curriculum learning is sensitive to the threshold parameter $D_0$. Figure \ref{fig:E1_curr_norway_loss} reveals that decreasing threshold value $D_0$, diminishes the effect of curriculum learning. Decreasing the threshold value results in a smaller pool of eligible examples that can be included in a stage 0 training set, which reduces the training set variability.  \todo{Oh my lordy}
\todo{Inexperienced teacher}
\todo{Smooth versus har switching. Increase in test loss}
\todo[inline]{Curriculum learning - similar to boosting. Only from the baseline training distribution and then harder, but from easier first and then to harder distributions. Active learning, disagreement?}

A bit surprising are the results from Experiment E7, which shows that training with the first stage only, did better than curriculum learning. This indicate that the inexperienced teacher model, actually did a good job separating out the very hard and possibly inconsistent examples from the first stage. It also shows that the second stage actually contains a good amount of inconsistent examples which penalize the network incorrectly, and affect the outcome. Alternatively, this might indicate that the threshold parameter $D_0$ is set to a value which results in enough example variation in the first stage training set. The network is therefore able to generalize to the task of road detection. However, excluding hard examples entirely from the training set should be done with caution. This could lead to hard, but correctly labelled examples to be excluded, as well as unfamiliar examples that the curriculum teacher has not seen before. If the curriculum teacher could accurately detect inconsistent labelling, there would be no reason for doing curriculum learning in the first place.\\ 

Another potential reason for the large spike in test loss after a stage switch, is that the entire training set is suddenly replaced. \\

In summary, the composition of the first stage has a big effect on the training outcome. The results demonstrate that the proposed curriculum strategy, which is based on estimating inconsistencies between labels and predictions, works well in practice.

\subsection{Performance of the Road Detection System}
The images in Figure \ref{fig:E6_performance} illustrate qualitatively the performance of the best network from Experiment E6. The prediction image in Figure \ref{fig:E6_model_predictions} has been stitched together from many $16 \times 16$ prediction patches. For this particular test image, the model was able to identify the majority of the roads present, except for an almost imperceptible dirt road on the right side of the image. There are also some prediction errors, such as roads being disconnected, and prediction artefacts in the forest areas. However, the majority of the forest artefacts have low prediction probabilities, and were removed by a threshold operation applied to the probabilities. The threshold value which result in the best precision and recall trade-off for the system, was used in this threshold operation.\\

An interesting observation is that the model also correctly predicts small private roads leading up to houses present in the image. Furthermore, the model detects construction roads in the upper left corner. Since these roads are not present in the label image, the model is penalized for making these predictions by the cross-entropy loss function.\\

The predictions errors are displayed in Figure \ref{fig:E6_hit_image}, where the road label pixels and road prediction pixels are superimposed on the aerial image. The road pixels that are coloured green, have been correctly predicted. Whereas, the red and blue coloured pixels show the prediction errors. The red pixels indicate areas in which the system failed to predict road, and the blue pixels show areas where the system incorrectly predicted road. Yet, the majority of the prediction errors are understandable, and arguably not actually errors at all. Most of the blue areas, covers pixels which depict asphalt surfaces, and some of the red areas have trees covering the road. However, a certain challenge is the amount of disconnected roads, which especially occur at road junctions and highway ramps. Possible reasons for this type of prediction errors can be the low frequency of junctions and ramps in the dataset or that the model capacity is inadequate. \\

\todo[inline]{Why results in other works is better. Model capacity most likely. Overlapping max pooling, non overlapping max poooling .Reduce the number of adjustable weights especially between the output of the third layer and the units of the hidden fully connected layer. Refer to tests}
\todo{Mention CRF here, post processing neural network. Clean up work}


\begin{figure}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\textwidth]{figs/E6/E6-label.jpg}
\caption{Label image.} \label{fig:E6_label_iamge}
\vspace{0.5cm} % separation vertically between the subfigures
\end{subfigure}
\hspace*{\fill} % separation between the subfigures
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\textwidth]{figs/E6/E6-image.jpg}
\caption{Aerial image.} \label{fig:E6_aerial_image}
\vspace{0.5cm} % separation vertically between the subfigures
\end{subfigure}

\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\textwidth]{figs/E6/E6-pred.jpg}
\caption{Model predictions.} \label{fig:E6_model_predictions}
\end{subfigure}
\hspace*{\fill} % separation between the subfigures
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\textwidth]{figs/E6/E6-hit.jpg}
\caption{Prediction hit and miss image.} \label{fig:E6_hit_image}
\end{subfigure}
\caption{E6 - Example of model's road detection performance. The aerial image is part of the test set in Massachusetts Roads Dataset} \label{fig:E6_performance}
\end{figure}

\todo[inline]{Mention limited amount of data. Curriculum learning advantageous for even bigger patch datasets?}
