\section{Experimental Design}
\label{sec:experimentalPlan}
To resolve the research questions and test the performance of the road detection system a number of experiments have been planned. Experiments related to the first research question include testing the robustness of the bootstrapping loss function compared to a baseline loss function. For the other research question regarding the performance of curriculum learning, the results from a network trained according to a curriculum strategy will be compared with a baseline network. This network is trained on a regular dataset with no particular example ordering. In addition, tests measuring the performance of the road detection system will be conducted, and the results compared to other works.\\

 Throughout the rest of this chapter the experiments will be referred to by their assigned id. An overview of all experiments and their assigned id, can be found in in Table \ref{tab:planned_experiments}.\\
\todo{Have a curriculum experiment without replacement. Include or not.}
\begin{table}[htp]
\caption{Experiment overview}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^l ^l ^p{5cm}}\hline
\rowstyle{\bfseries}
  ID & RQ & Dataset & Description\\\hline
  E1 & RQ2 & Norwegian Roads Dataset Vbase & Performance of curriculum learning at different thresholds $D_\theta$. \\
  E2 & RQ2 & Massachusetts Roads Dataset & Curriculum, baseline and anti-curriculum comparison. \\
  E3 & RQ1 & Massachusetts Roads Dataset & Bootstrapping versus baseline at different levels of label noise \\
  E4 & RQ1 & Norwegian Roads Dataset N50/Vbase & Performance of bootstrapping with a label set with large amounts of label noise \\
  E5 & RQ1 & Norwegian Roads Dataset Vbase & Bootstrapping loss performance at different levels of label noise\\
  E6 & - & Massachusetts Roads Dataset & Best performing road detection network \\
  E7 & RQ2 & Massachusetts Roads Dataset & Curriculum learning with a inexperienced teacher.\\
  E8 & - & Norwegian Roads Dataset N50/Vbase & Combining curriculum learning and bootstrapping. \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:planned_experiments}
\end{table}

Each experiment found in Table \ref{tab:planned_experiments} have been replicated and measured 10 times. The experiments have many sources of variability, and averaging the measurements improves the reliability of the result. The network, for instance, is initialized with random weights, and the patch dataset is constructed from random sampling. These factors influence the optimization process, and might create anomalous measurements.\todo{Need to check terminology}\\

The proposed methods have also been tested on two different datasets. Despite the datasets involving the same task of road segmentation, they do differ in many ways. The datasets depict separate aerial regions, have different \ac{GSD}, contain different topography and differs in label quality. Conducting experiments on both datasets, might verify whether the methods can be generally applicable or not.\todo{A bit strong i guess}\\

For all experiments, most hyperparameters are kept constant, and only the parameters related to the research questions are varied. These hyperparameters are presented in Section \ref{sec:experimentalSetup}. Furthermore, all regularization methods were enabled during testing. The proposed methods should provide some additional benefit when used in combination with regularizers. Considering that regularization methods already yields some robustness to label noise, other methods which claim to provide robustness towards label noise should be able to improve the model's performance even further \todo{Do they. Need cite! Bad sentence, but content is about right}.  \\

To show that the bootstrapping loss function is effective at handling inconsistent labelling, it will be compared with the cross-entropy loss function at different levels of label noise. To do so, inconsistent labels in the form of omission noise, are artificially introduced to the label images. The label degradation involves removing between 0\% and 40\% of the road class pixels from the label images, before sampling the patch datasets. The experiment will show how well the various loss functions cope with omission error, and whether bootstrapping loss function is more robust than the cross-entropy loss function. Artificially introducing omission noise is tested with both road datasets in Experiment E3 and E5.\\

The bootstrapping loss function is further tested in Experiment E4, where the training set consists of labels from the label set N50, while the more accurate label set Vbase is used for the validation and test set. The N50 label set contains a lot of naturally occurring registration error, which means that the robustness of the different loss functions can be tested by comparing the test losses.\\

The effectiveness of curriculum learning will be tested by training the network on two different patch dataset. The first is created according to a curriculum strategy, where each stage $\theta$ only contains examples with a difficulty estimate below $D_\theta$. The training distribution of each stage will increase in difficulty, which results in the network being gradually introduced to harder examples. The second patch dataset is created with no regard to the difficulty, by random sampling. Specifically, it is created with the difficulty threshold $D_\theta =1.0$ for all stages $\theta$. The performance from training the network on this dataset, forms the baseline which is compared to the performance of a network trained on the curriculum dataset. Both datasets contain the same number of stages, and therefore the same number of examples. The network configuration is also identical. Essentially, the only difference is the ordering of the examples in the patch datasets. Curriculum learning is tested with both aerial image datasets in Experiment E1 and E2.\\

The approach of curriculum learning also raises some other interesting questions. How well does a network perform if presented with the "harder" examples first? In Experiment E2, anti-curriculum learning is therefore tested. This resembles the teaching dimension principle, as discussed in Section \ref{sec:related_works}, in which harder examples closer to the decision boundary are presented first in order to resolve ambiguities quickly \todo{is it really called teaching dimension principle. More research here required}. Furthermore, Experiment E1 explores what happens when setting the difficulty threshold $D_\theta$ at different values.\\

In addition to presenting the experimental results as \ac{MSE} loss per epoch plots, the network performance is also presented as precision and recall curves. This is a common metric for evaluating road detection systems, as discussed in Section \ref{sec:background_theory}.\\

A precision and recall curve is created by thresholding the network's prediction probabilities by values between 0 and 1, and then computing the precision and recall using the binarized predictions and labels. The result is a curve that illustrates the trade-off between precision and recall. As the recall increases, the precision usually decreases. Similar to \citep{Mnih_aerial_images_noisy}, this thesis utilizes a relaxed measure of precision and recall, with a slack variable $p$ set to 3. The relaxed precision is denoted as the fraction of detected road pixels that are within $p$ pixels of a label road pixel. Whereas, the relaxed recall is defined as the fraction of true pixels that are within $p$ pixels of a detected pixel. All experiments listed in Table \ref{tab:planned_experiments} use the relaxed version of precision. However, only Experiment E6 use the relaxed recall measure, which means that most experiments have precision and recall values that are more conservative \todo{A bit awkward explanation. WHy not recall for the other stuff.}.\\

The reason for using the relaxed version of precision and recall is that the majority of the label maps exhibit a small amount of registration noise. It is therefore unreasonable to count predictions which are slightly off target as errors. Consequently, misalignments of 3 pixels or less between the prediction and and label will not affect the values of the relaxed precision and recall curve.\\



