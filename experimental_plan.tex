\section{Experimental Design}
\label{sec:experimentalPlan}
To resolve the research questions and test the performance of the road detection system, a number of experiments were planned. Experiments related to the first research question included testing the robustness of the bootstrapping loss function compared to a baseline loss function. For the other research question regarding the performance of curriculum learning, the results from a network trained according to a curriculum strategy was compared with a baseline network. The baseline network was trained using an ordinary dataset with no particular example ordering. In addition, tests measuring the performance of the road detection system were conducted, and the results compared to other works.\\

 Throughout the rest of this chapter the experiments will be referred to by their assigned id. An overview of all experiments and their assigned id, can be found in in Table \ref{tab:planned_experiments}.\\
\begin{table}[htp]
\caption[Experiments overview]{Experiments overview.}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^l ^l ^p{5cm}}\hline
\rowstyle{\bfseries}
  ID & RQ & Dataset & Description\\\hline
  
  
  E1 & RQ1 & Massachusetts Roads Dataset & Bootstrapping versus baseline at different levels of label noise \\
  E2 & RQ1 & Norwegian Roads Dataset Vbase & Bootstrapping versus baseline at different levels of label noise\\
  E3 & RQ1 & Norwegian Roads Dataset N50/Vbase & Performance of bootstrapping with a label set with large amounts of label noise \\
  E4 & RQ2 & Massachusetts Roads Dataset & Curriculum, baseline and anti-curriculum comparison. \\
  E5 & RQ2 & Norwegian Roads Dataset Vbase & Performance of curriculum learning at different thresholds $D_\theta$. \\
  E6 & RQ2 & Massachusetts Roads Dataset & Curriculum learning with an inexperienced teacher. \\
  E7 & - & Massachusetts Roads Dataset & Best performing road detection network. Larger patch datasets and increased model capacity. \\
  \hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:planned_experiments}
\end{table}

Each experiment found in Table \ref{tab:planned_experiments} has been replicated and measured 10 times. The experiments have many sources of variability, and averaging the measurements improves the reliability of the results. The network, for instance, is initialized with random weights, and the patch dataset is constructed from random sampling. These factors influence the optimization process, and create measurements that can fluctuate.\\

Most of the experiments listed in Table \ref{tab:planned_experiments} compares results from a certain method with a baseline. Running each experiment several times, creates two independent samples from these populations. By statistical hypothesis testing and the Welch's t test, it is possible to assert whether it is plausible that the samples were produced from two distinct populations. Population $i$ can be described by mean $\mu_i$ and variance $\sigma^2_i$. The null hypothesis $H_0\colon \mu_1 = \mu_2$ is rejected in favour of the alternate hypothesis $H_1\colon\mu_1 \neq \mu_2$ , based on the p-value found by the t-test.  If the p-value is below the significance level $\alpha$ of 0.05, $H_0$ is rejected. This indicates that it is unlikely that the samples came from the same underlying population.\\

The Welch's t test is derived from the Student's t test, and is suitable in situations where the variance and mean of the underlying populations are unknown \citep{walpole_probability}. The mean and variance are estimated from the samples. Unlike the Student's t test, the Welch's t test does not assume that the variance of the two populations are equal. However, both tests assume that the populations are normally distributed. This seemed to be true for the experiments in this chapter, based on visual interpretation of normal Q-Q plots created from the measurements. Unfortunately, whether or not the underlying populations were normal could not be confidently asserted because of the small sample size. In appendix -\todo{Create plot appendix}, examples of these plots can be found. \\ 
 
The proposed methods were tested on two different datasets. Despite the datasets involving the same task of road segmentation, they do differ in many ways. The datasets depict separate aerial regions, have different \ac{GSD}, contain different topography and differs in label quality. Conducting experiments on both datasets, might give an indication of whether the methods can be generally applicable or not.\\

For all experiments, most hyperparameters were kept constant. Only the parameters related to the research questions were varied. These hyperparameters are presented in Section \ref{sec:experimentalSetup}. Furthermore, all regularization methods were enabled during testing. The proposed methods should provide some additional benefit when used in combination with a typical configuration of a \ac{CNN}.\\

To show that the bootstrapping loss function is effective at handling inconsistent labelling, it was compared with the cross-entropy loss function at different levels of label noise. To do so, label errors in the form of omission noise, were artificially introduced to the label images. The label degradation involved removing between 0\% and 40\% of the road class pixels from the label images, before sampling the patch datasets. The road removal involved iteratively setting randomly sized regions of label pixels to 0. The experiment should show how well the various loss functions cope with omission noise, and whether bootstrapping loss function is more robust than the cross-entropy loss function. Artificially introducing omission noise was tested with both road datasets in Experiment E1 and E2\\

The bootstrapping loss function was further tested by Experiment E3, where the training set consisted of labels from the label set N50. The more accurate label set Vbase was used by the validation and test set. The N50 label set contains a lot of naturally occurring registration error, which means that the robustness of the different loss functions can be tested without artificial label degradation.\\

The effectiveness of curriculum learning was tested by training the network on two different patch datasets. The first was created according to a curriculum strategy, where each stage $\theta$ only contained examples with a difficulty estimate below $D_\theta$. Subsequent stages increases the difficulty threshold, which results in the network being gradually introduced to harder examples. The second patch dataset was created with no regard to the difficulty, by random sampling. Specifically, it was created with the difficulty threshold $D_\theta =1.0$ for all stages $\theta$. The performance from training the network on this dataset, formed the baseline which was compared to the performance of a network trained on the curriculum dataset. Both datasets contained the same number of stages, and therefore the same number of examples. The network configuration was also identical. Essentially, the only difference was the ordering of the examples in the patch datasets. Curriculum learning was tested with both aerial image datasets in Experiment E4 and E5.\\

The approach of curriculum learning also raises some other interesting questions. How well does a network perform if presented with the "harder" examples first? In Experiment E4, anti-curriculum learning was therefore tested. This resembles the teaching dimension method, as discussed in Section \ref{sec:related_works}, in which harder examples closer to the decision boundary are presented first in order to resolve ambiguities quickly. Furthermore, Experiment E5 explored what happens when setting the difficulty threshold $D_0$ at different values.\\

In Experiment E6, the curriculum strategy was tested with a less experienced curriculum teacher, than in Experiment E4 and E5. This should test whether the proposed curriculum strategy is viable with a teacher that has trained with a limited number of examples. The experiment also tested the effect of just training on the first stage examples. In addition, a more gradual approach of changing the training set distribution was tested. This involved having several smaller stages mixed into the training set by random replacement.\\

In addition to presenting the experimental results as \ac{MSE} test loss per epoch plots, the network performance is also presented as precision and recall curves. This is a common metric for evaluating road detection systems, as discussed in Section \ref{sec:background_theory}.\\

A precision and recall curve is created by thresholding the network's prediction probabilities by values between 0 and 1, and then computing the precision and recall using the binarized predictions and labels. The result is a curve that illustrates the trade-off between precision and recall. As the recall increases, the precision usually decreases. Similar to \citep{Mnih_aerial_images_noisy}, this thesis utilizes a relaxed measure of precision and recall, with a slack variable $p$ set to 3. The relaxed precision is denoted as the fraction of detected road pixels that are within $p$ pixels of a label road pixel. Whereas, the relaxed recall is defined as the fraction of true pixels that are within $p$ pixels of a detected pixel. All experiments listed in Table \ref{tab:planned_experiments} utilized the relaxed version of precision. However, only Experiment E6 and E7 recorded results using the relaxed recall measure, which means that most experiment results have precision and recall values that are a bit more sensitive to minor deviations between predictions and labels.\\

The reason for using the relaxed version of precision and recall is that the majority of the label maps exhibit a small amount of registration noise. It is therefore unreasonable to count predictions which are slightly off target as errors. Consequently, misalignments of 3 pixels or less between the prediction and label will not affect the values of the relaxed precision and recall curve.\\

The results are also presented in tables that include the precision and recall breakeven point, which is derived from the precision and recall curve. This is the point on the precision and recall curve where the precision and recall have an equal value. These points are also marked in the figures by a black dot.\\



