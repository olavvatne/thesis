\section{Experimental Design}
\label{sec:experimentalPlan}
To resolve the research questions and test the performance of the road detection system a number of experiments were planned. Experiments related to the first research question included testing the robustness of the bootstrapping loss function compared to a baseline loss function. For the other research question regarding the performance of curriculum learning, the results from a network trained according to a curriculum strategy was compared with a baseline network. This network was trained on a regular dataset with no particular example ordering. In addition, tests measuring the performance of the road detection system were conducted, and the results compared to other works.\\

 Throughout the rest of this chapter the experiments will be referred to by their assigned id. An overview of all experiments and their assigned id, can be found in in Table \ref{tab:planned_experiments}.\\
\todo{Have a curriculum experiment without replacement. Include or not.}
\begin{table}[htp]
\caption{Experiment overview}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{+l ^l ^l ^p{5cm}}\hline
\rowstyle{\bfseries}
  ID & RQ & Dataset & Description\\\hline
  E1 & RQ2 & Norwegian Roads Dataset Vbase & Performance of curriculum learning at different thresholds $D_\theta$. \\
  E2 & RQ2 & Massachusetts Roads Dataset & Curriculum, baseline and anti-curriculum comparison. \\
  E3 & RQ1 & Massachusetts Roads Dataset & Bootstrapping versus baseline at different levels of label noise \\
  E4 & RQ1 & Norwegian Roads Dataset N50/Vbase & Performance of bootstrapping with a label set with large amounts of label noise \\
  E5 & RQ1 & Norwegian Roads Dataset Vbase & Bootstrapping loss performance at different levels of label noise\\
  E6 & - & Massachusetts Roads Dataset & Best performing road detection network \\
  E7 & RQ2 & Massachusetts Roads Dataset & Curriculum learning with an inexperienced teacher. \\\hline
\end{tabular}
\end{adjustbox}
\end{center}
\label{tab:planned_experiments}
\end{table}

Each experiment found in Table \ref{tab:planned_experiments} has been replicated and measured 10 times. The experiments have many sources of variability, and averaging the measurements improves the reliability of the results. The network, for instance, is initialized with random weights, and the patch dataset is constructed from random sampling. These factors influence the optimization process, and might create anomalous measurements.\todo{Need to check terminology}\\

The proposed methods were also tested on two different datasets. Despite the datasets involving the same task of road segmentation, they do differ in many ways. The datasets depict separate aerial regions, have different \ac{GSD}, contain different topography and differs in label quality. Conducting experiments on both datasets, might give an indication of whether the methods can be generally applicable or not.\\

For all experiments, most hyperparameters were kept constant, and only the parameters related to the research questions were varied. These hyperparameters are presented in Section \ref{sec:experimentalSetup}. Furthermore, all regularization methods were enabled during testing. The proposed methods should provide some additional benefit when used in combination with regularizers. Considering that regularization methods already yields some robustness to label noise, other methods which claim to provide robustness should be able to improve the model's performance even further \todo{Do they. Need cite! Bad sentence, but content is about right}.  \\

To show that the bootstrapping loss function is effective at handling inconsistent labelling, it was compared with the cross-entropy loss function at different levels of label noise. To do so, inconsistent labels in the form of omission noise, were artificially introduced to the label images. The label degradation involved removing between 0\% and 40\% of the road class pixels from the label images, before sampling the patch datasets. The road removal involved iteratively setting randomly sized regions of label pixels to 0. The experiment should show how well the various loss functions cope with omission noise, and whether bootstrapping loss function is more robust than the cross-entropy loss function. Artificially introducing omission noise was tested with both road datasets in Experiment E3 and E5.\\

The bootstrapping loss function was further tested by Experiment E4, where the training set consisted of labels from the label set N50, while the more accurate label set Vbase was used in the validation and test set. The N50 label set contains a lot of naturally occurring registration error, which means that the robustness of the different loss functions can be tested without artificial label degradation.\\

The effectiveness of curriculum learning was tested by training the network on two different patch dataset. The first was created according to a curriculum strategy, where each stage $\theta$ only contained examples with a difficulty estimate below $D_\theta$. The training distribution of each stage increase in difficulty, which results in the network being gradually introduced to harder examples. The second patch dataset was created with no regard to the difficulty, by random sampling. Specifically, it was created with the difficulty threshold $D_\theta =1.0$ for all stages $\theta$. The performance from training the network on this dataset, formed the baseline which was compared to the performance of a network trained on the curriculum dataset. Both datasets contained the same number of stages, and therefore the same number of examples. The network configuration was also identical. Essentially, the only difference was the ordering of the examples in the patch datasets. Curriculum learning was tested with both aerial image datasets in Experiment E1 and E2.\\

The approach of curriculum learning also raises some other interesting questions. How well does a network perform if presented with the "harder" examples first? In Experiment E2, anti-curriculum learning was therefore tested. This resembles the teaching dimension principle, as discussed in Section \ref{sec:related_works}, in which harder examples closer to the decision boundary are presented first in order to resolve ambiguities quickly \todo{is it really called teaching dimension principle.}. Furthermore, Experiment E1 explored what happens when setting the difficulty threshold $D_\theta$ at different values.\\

In Experiment E7, the curriculum strategy was tested with a less experienced curriculum teacher, than in Experiment E1 and E2. This should test whether the proposed curriculum strategy is viable with a teacher that has trained with a limited number of examples. The experiment also tested the effect of just training on the first stage examples. In addition a more gradual approach of changing the training set distribution was tested. This involved having several smaller stages mixed into the training set by random replacement.\\

In addition to presenting the experimental results as \ac{MSE} loss per epoch plots, the network performance is also presented as precision and recall curves. This is a common metric for evaluating road detection systems, as discussed in Section \ref{sec:background_theory}.\\

A precision and recall curve is created by thresholding the network's prediction probabilities by values between 0 and 1, and then computing the precision and recall using the binarized predictions and labels. The result is a curve that illustrates the trade-off between precision and recall. As the recall increases, the precision usually decreases. Similar to \citep{Mnih_aerial_images_noisy}, this thesis utilizes a relaxed measure of precision and recall, with a slack variable $p$ set to 3. The relaxed precision is denoted as the fraction of detected road pixels that are within $p$ pixels of a label road pixel. Whereas, the relaxed recall is defined as the fraction of true pixels that are within $p$ pixels of a detected pixel. All experiments listed in Table \ref{tab:planned_experiments} utilized the relaxed version of precision. However, only Experiment E6 and E7 recorded results using the relaxed recall measure, which means that most experiment results have precision and recall values that are a bit more conservative than they ought to be.\todo{Reassure that the comparisons are correct.}\\

The reason for using the relaxed version of precision and recall is that the majority of the label maps exhibit a small amount of registration noise. It is therefore unreasonable to count predictions which are slightly off target as errors. Consequently, misalignments of 3 pixels or less between the prediction and and label will not affect the values of the relaxed precision and recall curve.\\



